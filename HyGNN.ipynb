{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HyGNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxm_OtRf5pA8"
      },
      "outputs": [],
      "source": [
        "!pip install karateclub\n",
        "!pip install ogb\n",
        "!pip install node2vec\n",
        "!pip install dgl\n",
        "!pip install hypernetx\n",
        "!pip install celluloid\n",
        "!pip install igraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "import hypernetx as hnx\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, f1_score, recall_score,precision_score, accuracy_score,average_precision_score,precision_recall_curve,auc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from karateclub import DeepWalk\n",
        "from node2vec import Node2Vec"
      ],
      "metadata": {
        "id": "7p59ZWYp5rTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "here, the hyG creation starts\n",
        "1. at frist we read the text file \"new_graph_data_1.txt\". This text file is created by running ESPF algorithm\n",
        "2. Then we create a dictionary 'DICT' that has 'drug' as key and drug's 'substructures' as values \n",
        "'''\n",
        "formattedNodes = []\n",
        "smileFile = open(\"drive/My Drive/Colab Notebooks/Hypergraph_my/new_graph_data_824_kmer.txt\", 'r') #ESPF or k-mer\n",
        "names = []\n",
        "\n",
        "for line in smileFile:\n",
        "    line = line.strip()\n",
        "    words = line.split(\" \")\n",
        "    name = words[0]\n",
        "    del words[0]\n",
        "    t=tuple(words)\n",
        "    names.append(name)\n",
        "    formattedNodes.append(t)\n",
        "      \n",
        "# Make the dictionary of the Drug IDs: Smiles tokens    \n",
        "DICT = dict()  \n",
        "for i in range(0, len(names)):\n",
        "    DICT[names[i]] = formattedNodes[i]    \n",
        "\n"
      ],
      "metadata": {
        "id": "0euEzWHo5uhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "feature one hot coding\n",
        "'''\n",
        "nl=np.zeros((824,824))\n",
        "np.fill_diagonal(nl, 1)\n",
        "drug_X=nl"
      ],
      "metadata": {
        "id": "O-s-soVU503B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Then we are using the DICT to create a hyG\n",
        "'''\n",
        "drug_list = [] # list of drug - translates drug name into index\n",
        "chemicalsub_list = [] # list of chemicalsub - translates chemicalsub number into index\n",
        "drug_chemicalsub = {} # dict of int:list - drug idx i wrote chemicalsubs with idx [j, k, l, .. ]\n",
        "#chemicalsub_drug = {} # dict of int:list - chemicalsub idx i were written by drugs with idx [j, k, l, ..]\n",
        "\n",
        "for drug in DICT.keys() :\n",
        "    chemicalsubs = DICT[drug]\n",
        "    if drug not in drug_list :\n",
        "        drug_list.append(drug)\n",
        "    idx = drug_list.index(drug)\n",
        "    if idx not in drug_chemicalsub :\n",
        "        drug_chemicalsub[idx]=[]   \n",
        "    \n",
        "    translated_p = []\n",
        "   \n",
        "    for chemicalsub in chemicalsubs :\n",
        "        if chemicalsub not in chemicalsub_list :\n",
        "            chemicalsub_list.append(chemicalsub)\n",
        "        p_idx = chemicalsub_list.index(chemicalsub)\n",
        "        translated_p.append(p_idx) # translate chemicalsub number into index\n",
        "  \n",
        "    drug_chemicalsub[idx]=translated_p\n",
        "\n",
        "'''\n",
        "chemicalsub : corresponds to hypernode\n",
        "citing : corresponds to hyperedge(drug)\n",
        "'''\n",
        "\n",
        "chemicalsub_citing = [] # list of [chemicalsub drug]\n",
        "n_chemicalsub = len(chemicalsub_list)\n",
        "n_hedge = len(drug_list)\n",
        "\n",
        "for drug in drug_chemicalsub.keys():\n",
        "    chemicalsubs = drug_chemicalsub[drug]\n",
        "    #if len(chemicalsubs) == 1 :\n",
        "        #continue # remove one-cited chemicalsubs\n",
        "    for chemicalsub in chemicalsubs :\n",
        "        chemicalsub_citing.append([chemicalsub, drug])\n",
        "       \n",
        "chemicalsub_drug = torch.LongTensor(chemicalsub_citing)\n",
        "data_dict = {\n",
        "        ('node', 'in', 'edge'): (chemicalsub_drug[:,0], chemicalsub_drug[:,1]),        \n",
        "        ('edge', 'con', 'node'): (chemicalsub_drug[:,1], chemicalsub_drug[:,0])\n",
        "    }\n",
        "\n",
        "\n",
        "'''\n",
        "finally passing the data_dict to construct hyG\n",
        "'''\n",
        "hyG = dgl.heterograph(data_dict)\n",
        "\n",
        "rows=n_chemicalsub\n",
        "columns=n_hedge\n",
        "\n",
        "'''\n",
        "reading the feature (one hot coding) for drugs (edges)\n",
        "'''\n",
        "\n",
        "#drug_X=np.loadtxt(\"drive/My Drive/Colab Notebooks/Hypergraph_my/features_one_hot_coding_824.txt\", dtype=int) #if you want to use 167 bits representation of each drug as feature read: features_bit.txt    \n",
        "hyG.ndata['h'] = {'edge' : torch.tensor(drug_X).type('torch.FloatTensor'), 'node' : torch.ones(rows, 100)}\n",
        "e_feat = torch.tensor(drug_X).type('torch.FloatTensor')\n",
        "v_feat = torch.ones(rows, 100)"
      ],
      "metadata": {
        "id": "MR6BP3A453Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "k5q3UwdT55jE",
        "outputId": "67112bad-e465-49ee-bc73-b33a544e7359"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-051262d73ffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhyG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'hyG' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The main regular graph creation (mat) from drugbank data, that will be used for train-test purpose.\n",
        "1. first we are creating a 2D matrix 'mat'\n",
        "2. writing the edge information in a text file\n",
        "3. reading the saved text file and extracting the src and dst node from that for each edge.\n",
        "These src, dst info will be used in DGL to create a dgl type graph\n",
        "\"\"\"\n",
        "\n",
        "#1:\n",
        "df=pd.read_csv(\"drive/My Drive/Colab Notebooks/Hypergraph_my/drug_and_smiles_824.csv\") \n",
        "mat = np.zeros((824, 824))\n",
        "df1=pd.read_csv(\"drive/My Drive/Colab Notebooks/Hypergraph_my/drugID_to_Integer_Mapping_824.txt\", header=None, sep=' ')\n",
        "D={}\n",
        "for i in range (len (df1)):\n",
        "    D[df1[1][i]]=df1[0][i]\n",
        "for i in range (len(df['Interacting Drugs'])):\n",
        "    SP=df['Interacting Drugs'][i]\n",
        "    SP=SP.split('; ')\n",
        "    SP1=list(SP)\n",
        "    for j in SP1:\n",
        "        for drug, id in D.items():\n",
        "            if j==drug:\n",
        "                mat[i,id]=mat[id,i]=1\n",
        "\n",
        "#np.count_nonzero(mat == 1)    \n",
        "\n",
        "#2:\n",
        "with open(\"drive/My Drive/Colab Notebooks/Hypergraph_my/edge_list_regular_graph_824.txt\", \"w\") as file_prime:\n",
        "  for i in range (len(mat)):\n",
        "    for j in range (len(mat)):\n",
        "      if mat[i,j] == 1.0:\n",
        "        stra=str(i)\n",
        "        strb=str(j)\n",
        "        strc=stra+'\\t'+strb+'\\n'\n",
        "        file_prime.write(strc)\n",
        "\n",
        "\n",
        "#3:\n",
        "src=[]\n",
        "dst=[]\n",
        "\n",
        "with open(\"drive/My Drive/Colab Notebooks/Hypergraph_my/edge_list_regular_graph_824.txt\") as fp:\n",
        "        for i,line in enumerate (fp):\n",
        "            info = line.strip().split()\n",
        "            src.append(info[0])\n",
        "            dst.append(info[1])\n",
        "\n",
        "src=np.asarray(src, dtype=np.int64) #to use in the data prerpocessing\n",
        "dst=np.asarray(dst, dtype=np.int64) #to use in the data prerpocessing\n"
      ],
      "metadata": {
        "id": "Se6PBHFz58tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "here, we are using src, dst info to create a DGL style graph. this graph (g) will be used for training and testing purpose.\n",
        "\n",
        "Source: DGL\n",
        "\"\"\"\n",
        "from dgl.data import DGLDataset\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class KarateClubDataset(DGLDataset):\n",
        "    def __init__(self):\n",
        "        super().__init__(name='karate_club')\n",
        "    def process(self):\n",
        "        edges_src = torch.from_numpy(src)\n",
        "        edges_dst = torch.from_numpy(dst)\n",
        "        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=824)\n",
        "    def __getitem__(self, i):\n",
        "        return self.graph\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "dataset = KarateClubDataset()\n",
        "g = dataset[0]\n",
        "print(g)"
      ],
      "metadata": {
        "id": "ADQODt_o5_kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Train, test (both positive and negative) graphs creation for training and testing purpose from the dgl graph (g)\n",
        "Source: DGL\n",
        "'''\n",
        "# Split edge set for training and testing\n",
        "u, v = g.edges()\n",
        "eids = np.arange(g.number_of_edges())\n",
        "eids = np.random.permutation(eids)\n",
        "eids=eids.tolist()\n",
        "test_size = int(len(eids) * 0.3)\n",
        "train_size = g.number_of_edges() - test_size\n",
        "test_size1=int(test_size/2)"
      ],
      "metadata": {
        "id": "Bw0hCHNa6BPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T1=[]\n",
        "T2=[]\n",
        "for i in eids:\n",
        "  m, n = int(u[i]), int(v[i])\n",
        "  c=(m,n)\n",
        "  T1.append(c)\n",
        "\n",
        "for i in T1:\n",
        "  m=i[0]\n",
        "  n=i[1]\n",
        "  c=(m,n)\n",
        "  d=(n,m)\n",
        "  if c not in T2 and d not in T2:\n",
        "    T2.append(c)\n",
        "  if len(T2)>=test_size1:\n",
        "    break  \n",
        "\n",
        "test_eids=[]\n",
        "for i in T2:\n",
        "  m= i[0]\n",
        "  n=i[1]\n",
        "  c=g.edge_id(m,n)\n",
        "  d=g.edge_id(n,m)\n",
        "  test_eids.append(c)\n",
        "  test_eids.append(d)\n",
        "\n",
        "train_eids=[]\n",
        "for i in range (len(eids)):\n",
        "  a=int(eids[i])\n",
        "  if a not in test_eids:\n",
        "    train_eids.append(a)  "
      ],
      "metadata": {
        "id": "JjmMlJJV6DRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pos_u, test_pos_v = u[test_eids], v[test_eids]\n",
        "train_pos_u, train_pos_v = u[train_eids], v[train_eids]\n",
        "\n",
        "# Find all negative edges and split them for training and testing\n",
        "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
        "adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
        "neg_u, neg_v = np.where(adj_neg != 0)\n",
        "\n",
        "neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n",
        "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
        "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n",
        "\n",
        "\n",
        "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())# this is just the edge_list\n",
        "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
        "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
        "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())"
      ],
      "metadata": {
        "id": "dXrZmK-W6KSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of edges: \",g.number_of_edges())\n",
        "print(\"num of train POSITIVE edges: \",train_pos_g.number_of_edges(),\", num of test POSITIVE edges: \",test_pos_g.number_of_edges())\n",
        "print(\"num of train NEGATIVE edges: \",train_neg_g.number_of_edges(),\", num of test NEGATIVE edges: \",test_neg_g.number_of_edges())"
      ],
      "metadata": {
        "id": "d3085lun6OI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_for_baseline = dgl.remove_edges(g, test_eids)\n",
        "#dgl.save_graphs(\"drive/My Drive/Colab Notebooks/Hypergraph_my/a.bin\",g_for_baseline)\n",
        "g_for_baseline = dgl.add_self_loop(g_for_baseline)"
      ],
      "metadata": {
        "id": "H0geewFH6QLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Decoder part. \n",
        "2 types of decoders.\n",
        "Anyone u can use.\n",
        "Source: DGL\n",
        "'''\n",
        "\n",
        "import dgl.function as fn\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h # here h is 822 drug features and g is the pos/neg train/test graph which is nothing but edge lis\n",
        "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
        "            return g.edata['score'][:, 0]\n",
        "\n",
        "class MLPPredictor(nn.Module):\n",
        "    def __init__(self, h_feats):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
        "        self.W2 = nn.Linear(h_feats, 1)\n",
        "\n",
        "    def apply_edges(self, edges):\n",
        "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
        "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            g.apply_edges(self.apply_edges)\n",
        "            return g.edata['score']\n",
        "\n",
        "\n",
        "decoder = MLPPredictor(100)\n",
        "#decoder = DotPredictor()# You can replace DotPredictor with MLPPredictor.\n",
        "#opt = torch.optim.Adam(list(model.parameters()) + list(pred.parameters()))"
      ],
      "metadata": {
        "id": "IjKE2ljK6SMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "compute_loss() and compute_auc() define\n",
        "'''\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "def compute_auc(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy() \n",
        "    precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
        "    auc_precision_recall = auc(recall, precision)\n",
        "    return roc_auc_score(labels, scores),auc(recall, precision)"
      ],
      "metadata": {
        "id": "DDNgmrPl6U4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.gat1 = HyperAttn_ne(drug_X.shape[1],64,100,100,0.5)\n",
        " \n",
        "    def forward(self,hyG, v_feat, e_feat,f,l):   \n",
        "        h = self.gat1(hyG, v_feat, e_feat,f,l)\n",
        "        return h\n",
        "model = Model()  "
      ],
      "metadata": {
        "id": "tdpfLLMF6WrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- 3. set up loss and optimizer -------------- #\n",
        "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), decoder.parameters()), lr=0.01)\n",
        "all_logits = []\n",
        "for e in range(600):\n",
        "    # forward\n",
        "    h=model(hyG, v_feat, e_feat,True,True)\n",
        "    h=h[1]\n",
        "    pos_score = decoder(train_pos_g, h)\n",
        "    neg_score = decoder(train_neg_g, h)\n",
        "    loss = compute_loss(pos_score, neg_score)\n",
        "    \n",
        "    # backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if e % 10 == 0:\n",
        "        print('In epoch {}, loss: {}'.format(e, loss))\n",
        "# ----------- 5. check results ------------------------ #\n",
        "\n",
        "with torch.no_grad():\n",
        "    pos_score = decoder(test_pos_g, h)\n",
        "    neg_score = decoder(test_neg_g, h)\n",
        "    c=compute_auc(pos_score, neg_score)\n",
        "    print('ROC-AUC: ', c[0],'PR-AUC: ',c[1])\n"
      ],
      "metadata": {
        "id": "nUFB51IH6Yuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = torch.cat([pos_score, neg_score])\n",
        "labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "\n",
        "m1 = tf.keras.metrics.BinaryAccuracy()\n",
        "m1.update_state(labels,scores)\n",
        "print(m1.result().numpy())\n",
        "\n",
        "sig_scores=F.sigmoid(scores)\n",
        "m2 = tf.keras.metrics.Precision()\n",
        "m2.update_state(labels,sig_scores)\n",
        "M2=m2.result().numpy()\n",
        "\n",
        "m3 = tf.keras.metrics.Recall()\n",
        "m3.update_state(labels,sig_scores)\n",
        "M3=m3.result().numpy()\n",
        "\n",
        "F1=2*(M2*M3)/(M2+M3)\n",
        "print(\"Precision:\",M2,\"Recall:\",M3,\"F1-score:\",F1)"
      ],
      "metadata": {
        "id": "NGQhBmVp6bkb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}