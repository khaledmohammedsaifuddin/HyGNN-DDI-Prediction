{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14012,"status":"ok","timestamp":1657292481091,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"MIvYbPoP73FF","outputId":"86b218d5-99a8-47ce-9cce-695cf9a6084b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oG9atGpX4RvG"},"outputs":[],"source":["!python '/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/EDA.py'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96b738wU8ME_"},"outputs":[],"source":["!pip install karateclub\n","!pip install node2vec\n","!pip install dgl"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8449,"status":"ok","timestamp":1657292510905,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"Qi47bKUP8PmO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aada26f7-623c-4b17-de6a-fa87a3f92c44"},"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]},{"output_type":"stream","name":"stderr","text":["Using backend: pytorch\n"]}],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import numba\n","from numba import jit,njit, cuda\n","import dgl\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import itertools\n","import numpy as np\n","import scipy.sparse as sp\n","import networkx as nx\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.metrics import roc_auc_score, f1_score, recall_score,precision_score, accuracy_score,average_precision_score,precision_recall_curve,auc\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import tree\n","from karateclub import DeepWalk\n","from node2vec import Node2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsANYDW28VLj"},"outputs":[],"source":["'''\n","here, the hyG creation starts\n","1. at frist we read the text file \"new_graph_data_1.txt\". This text file is created by running ESPF algorithm\n","2. Then we create a dictionary 'DICT' that has 'drug' as key and drug's 'substructures' as values \n","'''\n","formattedNodes = []\n","smileFile = open(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/new_graph_data_645_ESPF_5.txt\", 'r')\n","names = []\n","\n","for line in smileFile:\n","    line = line.strip()\n","    words = line.split(\" \")\n","    name = words[0]\n","    del words[0]\n","    t=tuple(words)\n","    names.append(name)\n","    formattedNodes.append(t)\n","      \n","# Make the dictionary of the Drug IDs: Smiles tokens    \n","DICT = dict()  \n","for i in range(0, len(names)):\n","    DICT[names[i]] = formattedNodes[i]  \n","LEN=len(DICT)      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AzqjDr3e_UPN"},"outputs":[],"source":["'''\n","converting to sparse matrix\n","'''\n","df=len(DICT)\n","from scipy.sparse import coo_matrix\n","nl=coo_matrix((df, df))\n","nl.setdiag(1)\n","#nl.toarray()\n","values = nl.data\n","indices = np.vstack((nl.row, nl.col))\n","i = torch.LongTensor(indices)\n","v = torch.FloatTensor(values)\n","shape = nl.shape\n","nl=torch.sparse_coo_tensor(i, v, torch.Size(shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1336,"status":"ok","timestamp":1657292695621,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"WnGVb55y9tsP","outputId":"5645a7f5-8e58-42f1-e15c-1ee130893b56"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Graph(num_nodes={'edge': 645, 'node': 555},\n","      num_edges={('edge', 'con', 'node'): 7547, ('node', 'in', 'edge'): 7547},\n","      metagraph=[('edge', 'node', 'con'), ('node', 'edge', 'in')])"]},"metadata":{},"execution_count":9}],"source":["'''\n","Then we are using the DICT to create a hyG\n","'''\n","drug_list = [] # list of drug - translates drug name into index\n","chemicalsub_list = [] # list of chemicalsub - translates chemicalsub number into index\n","drug_chemicalsub = {} # dict of int:list - drug idx i wrote chemicalsubs with idx [j, k, l, .. ]\n","#chemicalsub_drug = {} # dict of int:list - chemicalsub idx i were written by drugs with idx [j, k, l, ..]\n","numba.njit(target=\"cuda\")\n","def function(DICT):\n","  for drug in DICT.keys() :\n","      chemicalsubs = DICT[drug]\n","      if drug not in drug_list :\n","          drug_list.append(drug)\n","      idx = drug_list.index(drug)\n","      if idx not in drug_chemicalsub :\n","          drug_chemicalsub[idx]=[]   \n","      \n","      translated_p = []\n","    \n","      for chemicalsub in chemicalsubs :\n","          if chemicalsub not in chemicalsub_list :\n","              chemicalsub_list.append(chemicalsub)\n","          p_idx = chemicalsub_list.index(chemicalsub)\n","          translated_p.append(p_idx) # translate chemicalsub number into index\n","    \n","      drug_chemicalsub[idx]=translated_p\n","  return chemicalsub_list,drug_list,drug_chemicalsub    \n","function1=jit(parallel=True) (function)\n","chemicalsub_list,drug_list,drug_chemicalsub = function1(DICT)\n","\n","'''\n","chemicalsub : corresponds to hypernode\n","citing : corresponds to hyperedge(drug)\n","'''\n","\n","chemicalsub_citing = [] # list of [chemicalsub drug]\n","n_chemicalsub = len(chemicalsub_list)\n","n_hedge = len(drug_list)\n","\n","for drug in drug_chemicalsub.keys():\n","    chemicalsubs = drug_chemicalsub[drug]\n","    #if len(chemicalsubs) == 1 :\n","        #continue # remove one-cited chemicalsubs\n","    for chemicalsub in chemicalsubs : \n","        chemicalsub_citing.append([chemicalsub, drug])\n","       \n","chemicalsub_drug = torch.LongTensor(chemicalsub_citing)\n","data_dict = {\n","        ('node', 'in', 'edge'): (chemicalsub_drug[:,0], chemicalsub_drug[:,1]),        \n","        ('edge', 'con', 'node'): (chemicalsub_drug[:,1], chemicalsub_drug[:,0])\n","    }\n","\n","'''\n","finally passing the data_dict to construct hyG\n","'''\n","#hyG = dgl.heterograph(data_dict)\n","\n","\n","lst=[]\n","for i in chemicalsub_citing:\n","  lst.append(i[0])\n","s=set(lst)\n","s=len(s)  \n","num_nodes_dict = {'edge': LEN,'node':s}\n","hyG = dgl.heterograph(data_dict,num_nodes_dict=num_nodes_dict)\n","rows=n_chemicalsub\n","columns=n_hedge\n","'''\n","reading the feature (one hot coding) for drugs (edges)\n","'''\n","\n","#drug_X=np.loadtxt(\"/content/drive/MyDrive/My Drive/Colab Notebooks/DNA-Protein/DNA_features_one_hot_coding.txt\", dtype=int) #if you want to use 167 bits representation of each drug as feature read: features_bit.txt    \n","drug_X=nl\n","\n","v_feat=coo_matrix((rows, 128))\n","v_feat.setdiag(1)\n","#nl.toarray()\n","values = v_feat.data\n","indices = np.vstack((v_feat.row, v_feat.col))\n","i = torch.LongTensor(indices)\n","v = torch.FloatTensor(values)\n","shape = v_feat.shape\n","v_feat=torch.sparse_coo_tensor(i, v, torch.Size(shape))\n","\n","hyG.ndata['h'] = {'edge' : drug_X.type('torch.FloatTensor'), 'node' : v_feat.type('torch.FloatTensor')}\n","e_feat = drug_X.type('torch.FloatTensor')\n","v_feat=v_feat.type('torch.FloatTensor')\n","\n","hyG"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNxKlSPtLtck"},"outputs":[],"source":["'''\n","Don't run second time\n","'''\n","f = open( '/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/rows_drug_645_kmer_9.txt', 'w' )\n","f.write( 'num of rows = ' + str(rows) + '\\n')\n","f.close()\n","\n","torch.save(chemicalsub_drug, '/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/hyG_drug_645_kmer_9.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydFHKdf_L_MW"},"outputs":[],"source":["chemicalsub_drug = torch.load('/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/hyG_drug_645_kmer_9.pt')\n","data_dict = {\n","        ('node', 'in', 'edge'): (chemicalsub_drug[:,0], chemicalsub_drug[:,1]),        \n","        ('edge', 'con', 'node'): (chemicalsub_drug[:,1], chemicalsub_drug[:,0])\n","    }\n","'''\n","finally passing the data_dict to construct hyG\n","'''\n","hyG = dgl.heterograph(data_dict)\n","n_chemicalsub=14002\n","rows=n_chemicalsub\n","n_hedge=LEN\n","columns=n_hedge\n","'''\n","reading the feature (one hot coding) for drugs (edges)\n","'''\n","#drug_X=np.loadtxt(\"/content/drive/MyDrive/My Drive/Colab Notebooks/DNA-Protein/DNA_features_one_hot_coding.txt\", dtype=int) #if you want to use 167 bits representation of each drug as feature read: features_bit.txt    \n","drug_X=nl\n","hyG.ndata['h'] = {'edge' : torch.tensor(drug_X).type('torch.FloatTensor'), 'node' : torch.ones(rows, 128)}\n","e_feat = torch.tensor(drug_X).type('torch.FloatTensor')\n","v_feat = torch.ones(rows, 128)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":98,"status":"ok","timestamp":1657272342649,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"cModgoYc3WxK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0be8ce86-e40e-404b-e4b6-6d5f0acf98ac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Graph(num_nodes={'edge': 645, 'node': 555},\n","      num_edges={('edge', 'con', 'node'): 7547, ('node', 'in', 'edge'): 7547},\n","      metagraph=[('edge', 'node', 'con'), ('node', 'edge', 'in')])"]},"metadata":{},"execution_count":168}],"source":["hyG"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PhiJHdxrGtd8"},"outputs":[],"source":["\"\"\"\n","*** for TOWSIDES (645) ddi data *******\n","The main regular graph creation (mat) from drugbank data, that will be used for train-test purpose.\n","1. first we are creating a 2D matrix 'mat'\n","2. writing the edge information in a text file\n","3. reading the saved text file and extracting the src and dst node from that for each edge.\n","These src, dst info will be used in DGL to create a dgl type graph\n","\"\"\"\n","\n","#1:\n","df=pd.read_csv(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/TDC_TWOSIDES_useable.csv\",header=None) \n","cols=['Drug1_ID','Drug1','Drug2_ID','Drug2']\n","df1=pd.read_csv(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/TDC_TWOSIDES.csv\",usecols=cols) \n","df1 = df1.drop_duplicates(subset=cols)\n","df1.reset_index(drop=True, inplace=True)\n","\n","D={}\n","for i in range (len (df)):\n","    D[df[0][i]]=i\n","\n","drug1_lst=df1['Drug1_ID'].tolist()\n","drug2_lst=df1['Drug2_ID'].tolist()\n","LEN1=len(df1['Drug1_ID'])\n","mat = np.zeros((LEN, LEN))\n","numba.njit(target=\"cuda\")\n","def func(D,mat):\n","  for i in range (LEN1):\n","          for drug, id in D.items():\n","              if drug==drug1_lst[i]:\n","                  x=id \n","              elif drug==drug2_lst[i]:\n","                  y=id \n","              elif x!=-1 and y!=-1:\n","                break      \n","          print(x,y)\n","          mat[x,y]=mat[y,x]=1\n","          x=y=-1\n","  return mat                     \n","function1=jit(parallel=True) (func)\n","mat = function1(D,mat)\n","\n","#np.count_nonzero(mat == 1)    \n","'''\n","#2:\n","with open(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/edge_list_regular_graph_1706.txt\", \"w\") as file_prime:\n","  for i in range (len(mat)):\n","    for j in range (len(mat)):\n","      if mat[i,j] == 1.0:\n","        stra=str(i)\n","        strb=str(j)\n","        strc=stra+'\\t'+strb+'\\n'\n","        file_prime.write(strc)\n","\n","\n","#3:\n","src=[]\n","dst=[]\n","\n","with open(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/edge_list_regular_graph_1706.txt\") as fp:\n","        for i,line in enumerate (fp):\n","            info = line.strip().split()\n","            src.append(info[0])\n","            dst.append(info[1])\n","\n","src=np.asarray(src, dtype=np.int64) #to use in the data prerpocessing\n","dst=np.asarray(dst, dtype=np.int64) #to use in the data prerpocessing\n","'''"]},{"cell_type":"code","source":["len(df1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mVOu4KxHWmZw","executionInfo":{"status":"ok","timestamp":1657292879030,"user_tz":300,"elapsed":106,"user":{"displayName":"kms hamim","userId":"17071813694682210730"}},"outputId":"7b580d3f-a0c4-46f6-9999-d39c2e950973"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["63473"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vR3bvhsYjS6y"},"outputs":[],"source":["#3:\n","src=[]\n","dst=[]\n","\n","with open(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/edge_list_regular_graph_645.txt\") as fp:\n","        for i,line in enumerate (fp):\n","            info = line.strip().split()\n","            src.append(info[0])\n","            dst.append(info[1])\n","\n","src=np.asarray(src, dtype=np.int64) #to use in the data prerpocessing\n","dst=np.asarray(dst, dtype=np.int64) #to use in the data prerpocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9zDeFMuiP9cY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657292829350,"user_tz":300,"elapsed":88,"user":{"displayName":"kms hamim","userId":"17071813694682210730"}},"outputId":"36f80913-83c6-453b-b199-89bdef0b25f9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["126946"]},"metadata":{},"execution_count":13}],"source":["np.count_nonzero(mat == 1) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztMBUl6I8ojJ"},"outputs":[],"source":["\"\"\"\n","*** for old 824 ddi data *******\n","The main regular graph creation (mat) from drugbank data, that will be used for train-test purpose.\n","1. first we are creating a 2D matrix 'mat'\n","2. writing the edge information in a text file\n","3. reading the saved text file and extracting the src and dst node from that for each edge.\n","These src, dst info will be used in DGL to create a dgl type graph\n","\"\"\"\n","\n","#1:\n","df=pd.read_csv(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/drug_and_smiles_824.csv\") \n","mat = np.zeros((824, 824))\n","df1=pd.read_csv(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/drugID_to_Integer_Mapping_824.txt\", header=None, sep=' ')\n","D={}\n","for i in range (len (df1)):\n","    D[df1[1][i]]=df1[0][i]\n","for i in range (len(df['Interacting Drugs'])):\n","    SP=df['Interacting Drugs'][i]\n","    SP=SP.split('; ')\n","    SP1=list(SP)\n","    for j in SP1:\n","        for drug, id in D.items():\n","            if j==drug:\n","                mat[id,i]=mat[i,id]=1\n","\n","#np.count_nonzero(mat == 1)    \n","\n","#2:\n","with open(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/edge_list_regular_graph_824.txt\", \"w\") as file_prime:\n","  for i in range (len(mat)):\n","    for j in range (len(mat)):\n","      if mat[i,j] == 1.0:\n","        stra=str(i)\n","        strb=str(j)\n","        strc=stra+'\\t'+strb+'\\n'\n","        file_prime.write(strc)\n","\n","\n","#3:\n","src=[]\n","dst=[]\n","\n","with open(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/edge_list_regular_graph_824.txt\") as fp:\n","        for i,line in enumerate (fp):\n","            info = line.strip().split()\n","            src.append(info[0])\n","            dst.append(info[1])\n","\n","src=np.asarray(src, dtype=np.int64) #to use in the data prerpocessing\n","dst=np.asarray(dst, dtype=np.int64) #to use in the data prerpocessing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120,"status":"ok","timestamp":1657272350611,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"KYhYK_AT82v0","outputId":"947f3e8b-6602-4c39-904c-bad7b31dac64"},"outputs":[{"output_type":"stream","name":"stdout","text":["Graph(num_nodes=645, num_edges=126946,\n","      ndata_schemes={}\n","      edata_schemes={})\n"]}],"source":["\"\"\"\n","here, we are using src, dst info to create a DGL style graph. this graph (g) will be used for training and testing purpose.\n","\n","Source: DGL\n","\"\"\"\n","from dgl.data import DGLDataset\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class KarateClubDataset(DGLDataset):\n","    def __init__(self):\n","        super().__init__(name='karate_club')\n","    def process(self):\n","        edges_src = torch.from_numpy(src)\n","        edges_dst = torch.from_numpy(dst)\n","        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=LEN)\n","    def __getitem__(self, i):\n","        return self.graph\n","    def __len__(self):\n","        return 1\n","\n","dataset = KarateClubDataset()\n","g = dataset[0]\n","print(g)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3195,"status":"ok","timestamp":1657272362639,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"T8jDitU0B9TA","outputId":"4880eb1d-59b8-4b5e-f74b-49023bc4c2d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of edges:  126946\n","num of train POSITIVE edges:  107588 , num of test POSITIVE edges:  17358 num of val POSITIVE edges:  2000\n","num of train NEGATIVE edges:  114308 , num of test NEGATIVE edges:  10638 num of val NEGATIVE edges:  2000\n"]}],"source":["########################################## for new drugs #########################################\n","np.random.seed(42)\n","u, v = g.edges()\n","nids = np.arange(g.number_of_nodes())\n","#nids = np.random.permutation(nids)\n","\n","val_size=2000\n","\n","test_size = int(len(nids) * 0.05)\n","train_size = g.number_of_nodes() - test_size\n","\n","eids = np.arange(g.number_of_edges())\n","eids = np.random.permutation(eids)\n","\n","test_pos_eids = [i for i in eids if u[i]<=test_size or v[i]<=test_size]\n","test_pos_u, test_pos_v = u[test_pos_eids[:]], v[test_pos_eids[:]]\n","\n","train_pos_eids = [i for i in eids if u[i]>test_size and v[i]>test_size]\n","train_pos_u, train_pos_v = u[train_pos_eids[:]], v[train_pos_eids[:]]\n","val_pos_u,val_pos_v=train_pos_u[:val_size], train_pos_v[:val_size]\n","train_pos_u, train_pos_v = train_pos_u[val_size:], train_pos_v[val_size:]\n","\n","\n","adj = sp.coo_matrix((np.ones(len(u)), (u.cpu().numpy(), v.cpu().numpy())))\n","adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n","neg_u, neg_v = np.where(adj_neg != 0)\n","neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n","\n","test_neg_eids = [i for i in neg_eids if neg_u[i]<=test_size or neg_v[i]<=test_size]\n","test_neg_u, test_neg_v = neg_u[test_neg_eids[:]], neg_v[test_neg_eids[:]]\n","'''\n","if (len(test_neg_u)) != len(test_pos_u):\n","  if len(test_pos_u)> len(test_neg_u):\n","    a=len(test_pos_u)\n","    b=len(test_pos_u)-len(test_neg_u)\n","    test_pos_u=test_pos_u[:a-b]\n","    test_pos_v=test_pos_v[:a-b]\n","    \n","  if len(test_pos_u)< len(test_neg_u):\n","    a=len(test_neg_u)\n","    b=len(test_neg_u)-len(test_pos_u)\n","    test_neg_u=test_neg_u[:a-b]\n","    test_neg_v=test_neg_v[:a-b]  \n","'''\n","\n","train_neg_eids = [i for i in neg_eids if neg_u[i]>test_size and neg_v[i]>test_size]\n","train_neg_u, train_neg_v = neg_u[train_neg_eids[:]], neg_v[train_neg_eids[:]]\n","val_neg_u,val_neg_v=train_neg_u[:val_size], train_neg_v[:val_size]\n","train_neg_u, train_neg_v = train_neg_u[val_size:], train_neg_v[val_size:]\n","'''\n","if (len(train_neg_u)) != len(train_pos_u):\n","  if len(train_pos_u)> len(train_neg_u):\n","    a=len(train_pos_u)\n","    b=len(train_pos_u)-len(train_neg_u)\n","    train_pos_u=train_pos_u[:a-b]\n","    train_pos_v=train_pos_v[:a-b]\n","    \n","  if len(train_pos_u)< len(train_neg_u):\n","    a=len(train_neg_u)\n","    b=len(train_neg_u)-len(train_pos_u)\n","    train_neg_u=train_neg_u[:a-b]\n","    train_neg_v=train_neg_v[:a-b]  \n","'''\n","\n","train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())# this is just the edge_list\n","train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n","test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n","test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n","val_pos_g = dgl.graph((val_pos_u, val_pos_v), num_nodes=g.number_of_nodes())\n","val_neg_g = dgl.graph((val_neg_u, val_neg_v), num_nodes=g.number_of_nodes())\n","\n","print(\"Total number of edges: \",g.number_of_edges())\n","print(\"num of train POSITIVE edges: \",train_pos_g.number_of_edges(),\", num of test POSITIVE edges: \",test_pos_g.number_of_edges(),\"num of val POSITIVE edges: \",val_pos_g.number_of_edges())\n","print(\"num of train NEGATIVE edges: \",train_neg_g.number_of_edges(),\", num of test NEGATIVE edges: \",test_neg_g.number_of_edges(),\"num of val NEGATIVE edges: \",val_neg_g.number_of_edges())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1657117457815,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"CBxKQhGIxD6Z","outputId":"8c3e1163-4598-4376-fe37-8bbb7d21887e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of edges:  382804\n","num of train POSITIVE edges:  38281 , num of test POSITIVE edges:  306243 num of val POSITIVE edges:  38280\n","num of train NEGATIVE edges:  38281 , num of test NEGATIVE edges:  306243 num of val NEGATIVE edges:  38280\n"]}],"source":["# Split edge set for training and testing\n","u, v = g.edges()\n","np.random.seed(42)\n","eids = np.arange(g.number_of_edges())\n","eids = np.random.permutation(eids)\n","test_size = int(len(eids) * 0.8)\n","val_size = int(len(eids) * 0.1)\n","train_size=len(eids)-(test_size+val_size)\n","\n","#train_size = g.number_of_edges() - test_size\n","test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n","val_pos_u, val_pos_v = u[eids[test_size:test_size+val_size]], v[eids[test_size:test_size+val_size]]\n","train_pos_u, train_pos_v = u[eids[test_size+val_size:]], v[eids[test_size+val_size:]]\n","\n","# Find all negative edges and split them for training and testing\n","adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n","adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n","neg_u, neg_v = np.where(adj_neg != 0)\n","\n","neg_eids = np.random.choice(len(neg_u), g.number_of_edges(),replace=False)\n","test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n","val_neg_u, val_neg_v = neg_u[neg_eids[test_size:test_size+val_size]], neg_v[neg_eids[test_size:test_size+val_size]]\n","train_neg_u, train_neg_v = neg_u[neg_eids[test_size+val_size:]], neg_v[neg_eids[test_size+val_size:]]\n","\n","train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())# this is just the edge_list\n","train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n","test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n","test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n","val_pos_g = dgl.graph((val_pos_u, val_pos_v), num_nodes=g.number_of_nodes())\n","val_neg_g = dgl.graph((val_neg_u, val_neg_v), num_nodes=g.number_of_nodes())\n","\n","print(\"Total number of edges: \",g.number_of_edges())\n","print(\"num of train POSITIVE edges: \",train_pos_g.number_of_edges(),\", num of test POSITIVE edges: \",test_pos_g.number_of_edges(),\"num of val POSITIVE edges: \",val_pos_g.number_of_edges())\n","print(\"num of train NEGATIVE edges: \",train_neg_g.number_of_edges(),\", num of test NEGATIVE edges: \",test_neg_g.number_of_edges(),\"num of val NEGATIVE edges: \",val_neg_g.number_of_edges())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176375,"status":"ok","timestamp":1657203587346,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"TOecEkgG_Dvq","outputId":"0ce9dd42-59f2-4029-efe9-482496504ca4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of edges:  288434\n","num of train POSITIVE edges:  101558 , num of test POSITIVE edges:  12694 num of val POSITIVE edges:  12694\n","num of train NEGATIVE edges:  101558 , num of test NEGATIVE edges:  12694 num of val NEGATIVE edges:  12694\n"]}],"source":["'''\n","*************another approach\n","Train, test (both positive and negative) graphs creation for training and testing purpose from the dgl graph (g)\n","Source: DGL\n","'''\n","np.random.seed(42)\n","# Split edge set for training and testing\n","u, v = g.edges()\n","eids = np.arange(g.number_of_edges())\n","eids = np.random.permutation(eids)\n","eids=eids.tolist()\n","test_size = int(len(eids) * 0.1)\n","val_size = int(len(eids) * 0.1)\n","train_size=len(eids)-(test_size+val_size)\n","test_size1=int(test_size/2)\n","\n","\n","T1=[]\n","T2=[]\n","numba.njit(target=\"cuda\")\n","def func2(T1):\n","  for i in eids:\n","    m, n = int(u[i]), int(v[i])\n","    c=(m,n)\n","    T1.append(c)\n","  return T1  \n","function1=jit(parallel=True) (func2)\n","T1 = function1(T1)\n","\n","\n","numba.njit(target=\"cuda\")\n","def func3(T1,T2):\n","  for i in T1:\n","    m=i[0]\n","    n=i[1]\n","    c=(m,n)\n","    d=(n,m)\n","    if c not in T2 and d not in T2:\n","      T2.append(c)\n","    if len(T2)>=test_size1:\n","      break  \n","  return T2\n","function1=jit(parallel=True) (func3)\n","T2 = function1(T1,T2)\n","\n","\n","test_eids=[]\n","numba.njit(target=\"cuda\")\n","def func4(T2,test_eids,g):\n","  for i in T2:\n","    m= i[0]\n","    n=i[1]\n","    c=g.edge_id(m,n)\n","    d=g.edge_id(n,m)\n","    test_eids.append(c)\n","    test_eids.append(d)\n","  return test_eids  \n","function1=jit(parallel=True) (func4)\n","test_eids = function1(T2,test_eids,g)\n","\n","val_eids=[]\n","numba.njit(target=\"cuda\")\n","def func5(eids,test_eids,val_eids):\n","  i=0\n","  for j in eids:\n","    a=int(eids[i])\n","    if a not in test_eids and len(val_eids)<val_size:\n","      val_eids.append(a)  \n","    i=i+1\n","  return val_eids    \n","function1=jit(parallel=True) (func5)\n","val_eids = function1(eids,test_eids,val_eids)     \n","\n","train_eids=[]\n","numba.njit(target=\"cuda\")\n","def func6(eids,test_eids,val_eids,train_eids):\n","  i=0\n","  for j in eids:\n","    a=int(eids[i])\n","    if a not in test_eids and a not in val_eids and len(train_eids)<train_size:\n","      train_eids.append(a)  \n","    i=i+1\n","  return train_eids    \n","function1=jit(parallel=True) (func6)\n","train_eids = function1(eids,test_eids,val_eids,train_eids)     \n","\n","# main one. splitting into train and test set.\n","test_pos_u, test_pos_v = u[test_eids], v[test_eids]\n","train_pos_u, train_pos_v = u[train_eids], v[train_eids]\n","val_pos_u, val_pos_v = u[val_eids], v[val_eids]\n","\n","\n","# Find all negative edges and split them for training and testing\n","adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n","adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n","neg_u, neg_v = np.where(adj_neg != 0)\n","\n","neg_eids = np.random.choice(len(neg_u), g.number_of_edges(),replace=False)\n","g_=nx.from_numpy_matrix(adj_neg)\n","g_=dgl.from_networkx(g_)\n","\n","T1=[]\n","T2=[]\n","numba.njit(target=\"cuda\")\n","def func2(T1):\n","  for i in neg_eids:\n","    m, n = int(neg_u[i]), int(neg_v[i])\n","    c=(m,n)\n","    T1.append(c)\n","  return T1  \n","function1=jit(parallel=True) (func2)\n","T1 = function1(T1)\n","\n","\n","numba.njit(target=\"cuda\")\n","def func3(T1,T2):\n","  for i in T1:\n","    m=i[0]\n","    n=i[1]\n","    c=(m,n)\n","    d=(n,m)\n","    if c not in T2 and d not in T2:\n","      T2.append(c)\n","    if len(T2)>=test_size1:\n","      break  \n","  return T2\n","function1=jit(parallel=True) (func3)\n","T2 = function1(T1,T2)\n","\n","\n","test_neg_eids=[]\n","numba.njit(target=\"cuda\")\n","def func4(T2,test_eids,g):\n","  for i in T2:\n","    m= i[0]\n","    n=i[1]\n","    c=g.edge_id(m,n)\n","    d=g.edge_id(n,m)\n","    test_neg_eids.append(c)\n","    test_neg_eids.append(d)\n","  return test_neg_eids  \n","function1=jit(parallel=True) (func4)\n","test_neg_eids = function1(T2,test_neg_eids,g_)\n","\n","val_neg_eids=[]\n","numba.njit(target=\"cuda\")\n","def func5(neg_eids,test_neg_eids,val_neg_eids):\n","  i=0\n","  for j in neg_eids:\n","    a=int(neg_eids[i])\n","    if a not in test_neg_eids and len(val_neg_eids)<(val_size):\n","      val_neg_eids.append(a)  \n","    i=i+1\n","  return val_neg_eids    \n","function1=jit(parallel=True) (func5)\n","val_neg_eids = function1(neg_eids,test_neg_eids,val_neg_eids)  \n","\n","train_neg_eids=[]\n","numba.njit(target=\"cuda\")\n","def func6(neg_eids,test_neg_eids,val_neg_eids,train_neg_eids):\n","  i=0\n","  for j in neg_eids:\n","    a=int(neg_eids[i])\n","    if a not in test_neg_eids and a not in val_neg_eids and len(train_neg_eids)<(train_size):\n","      train_neg_eids.append(a)  \n","    i=i+1\n","  return train_neg_eids    \n","function1=jit(parallel=True) (func6)\n","train_neg_eids = function1(neg_eids,test_neg_eids,val_neg_eids,train_neg_eids)     \n","\n","test_neg_u, test_neg_v = neg_u[test_neg_eids], neg_v[test_neg_eids]\n","train_neg_u, train_neg_v = neg_u[train_neg_eids], neg_v[train_neg_eids]\n","val_neg_u, val_neg_v = neg_u[val_neg_eids], neg_v[val_neg_eids]\n","\n","train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())# this is just the edge_list\n","train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n","test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n","test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n","val_pos_g = dgl.graph((val_pos_u, val_pos_v), num_nodes=g.number_of_nodes())\n","val_neg_g = dgl.graph((val_neg_u, val_neg_v), num_nodes=g.number_of_nodes())\n","\n","print(\"Total number of edges: \",g_.number_of_edges())\n","print(\"num of train POSITIVE edges: \",train_pos_g.number_of_edges(),\", num of test POSITIVE edges: \",test_pos_g.number_of_edges(),\"num of val POSITIVE edges: \",val_pos_g.number_of_edges())\n","print(\"num of train NEGATIVE edges: \",train_neg_g.number_of_edges(),\", num of test NEGATIVE edges: \",test_neg_g.number_of_edges(),\"num of val NEGATIVE edges: \",val_neg_g.number_of_edges())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WoqvTtA0E45"},"outputs":[],"source":["'''\n","Data prep for CASTER. This part returns negative edges\n","'''\n","\n","neg_u, neg_v\n","neg_u_s=[]\n","neg_v_s=[]\n","\n","numba.njit(target=\"cuda\")\n","def func(neg_u):\n","  for i in neg_u:\n","    for j,k in D.items():\n","      if i==k:\n","        neg_u_s.append(j)\n","        print('o')\n","        break\n","  return neg_u_s      \n","function1=jit(parallel=True) (func)\n","neg_u_s = function1(neg_u) \n","\n","numba.njit(target=\"cuda\")\n","def func(neg_v):\n","  for i in neg_v:\n","    for j,k in D.items():\n","      if i==k:\n","        neg_v_s.append(j)\n","        print('o')\n","        break\n","  return neg_v_s      \n","function1=jit(parallel=True) (func)\n","neg_v_s = function1(neg_v)   \n","\n","D_1={}\n","for i in range (len(df[1])):\n","    D_1[df[0][i]]=df[1][i]\n","\n","neg_u_s,neg_v_s\n","neg_u_s1=[]\n","neg_v_s1=[]\n","\n","numba.njit(target=\"cuda\")\n","def func(neg_u_s):\n","  for i in neg_u_s:\n","    for j,k in D_1.items():\n","      if i==j:\n","        neg_u_s1.append(k)\n","        print('o')\n","        break\n","  return neg_u_s1     \n","function1=jit(parallel=True) (func)\n","neg_u_s1 = function1(neg_u_s) \n","\n","numba.njit(target=\"cuda\")\n","def func(neg_v_s):\n","  for i in neg_v_s:\n","    for j,k in D_1.items():\n","      if i==j:\n","        neg_v_s1.append(k)\n","        print('o')\n","        break\n","  return neg_v_s1      \n","function1=jit(parallel=True) (func)\n","neg_v_s1 = function1(neg_v_s)  \n","\n","LEN=len(u)\n","a_list_neg=[0]*LEN\n","len(a_list_neg)\n","\n","df_re_reg = pd.DataFrame(list(zip(neg_u_s, neg_u_s1,neg_v_s,neg_v_s1,a_list_neg)),columns =['Drug1_ID', 'Drug1_SMILES','Drug2_ID','Drug2_SMILES','label'])\n","df_re_reg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ls--9GqK31S0"},"outputs":[],"source":["'''\n","Data prep for CASTER. This part returns positive edges\n","'''\n","u,v\n","u_s=[]\n","v_s=[]\n","\n","numba.njit(target=\"cuda\")\n","def func(u):\n","  for i in u:\n","    for j,k in D.items():\n","      if i==k:\n","        u_s.append(j)\n","        print('o')\n","        break\n","  return u_s      \n","function1=jit(parallel=True) (func)\n","u_s = function1(u) \n","\n","numba.njit(target=\"cuda\")\n","def func(v):\n","  for i in v:\n","    for j,k in D.items():\n","      if i==k:\n","        v_s.append(j)\n","        print('o')\n","        break\n","  return v_s      \n","function1=jit(parallel=True) (func)\n","v_s = function1(v)   \n","\n","D_1={}\n","for i in range (len(df[1])):\n","    D_1[df[0][i]]=df[1][i]\n","\n","u_s,v_s\n","u_s1=[]\n","v_s1=[]\n","\n","numba.njit(target=\"cuda\")\n","def func(u_s):\n","  for i in u_s:\n","    for j,k in D_1.items():\n","      if i==j:\n","        u_s1.append(k)\n","        print('o')\n","        break\n","  return u_s1     \n","function1=jit(parallel=True) (func)\n","u_s1 = function1(u_s) \n","\n","numba.njit(target=\"cuda\")\n","def func(v_s):\n","  for i in v_s:\n","    for j,k in D_1.items():\n","      if i==j:\n","        v_s1.append(k)\n","        print('o')\n","        break\n","  return v_s1      \n","function1=jit(parallel=True) (func)\n","v_s1 = function1(v_s)  \n","\n","LEN=len(u)\n","a_list_pos=[1]*LEN\n","len(a_list_pos)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4221,"status":"ok","timestamp":1656642574121,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"MLUkDmIl9Yo1","outputId":"995c91e5-4cae-453d-f096-2d9abf121d93"},"outputs":[{"name":"stdout","output_type":"stream","text":["382804\n"]}],"source":["df_re = pd.DataFrame(list(zip(u_s, u_s1,v_s,v_s1,a_list_pos)),columns =['Drug1_ID', 'Drug1_SMILES','Drug2_ID','Drug2_SMILES','label'])\n","print(len(df_re_reg))\n","df_re = df_re.append(df_re_reg, ignore_index=True)\n","df_re = df_re.sample(frac=1).reset_index(drop=True)\n","df_re.to_csv('/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/TDC_DrugBank_FOR_CASTER.csv',index=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGp6OfMpPkHG"},"outputs":[],"source":["'''\n","when u want to exclude some nodes' edge (DDI) from trainset. You can do this in one of three ways:1. using a for loop 2. using a list of those node id \n","3. or directly putting those drugs' edges in 'testset (pos/neg), that are excluded form trainset \n","'''\n","'''\n","test_pos_u, test_pos_v = u[test_eids], v[test_eids]\n","train_pos_u, train_pos_v = u[train_eids], v[train_eids]\n","val_pos_u, val_pos_v = u[val_eids], v[val_eids]\n","'''\n","print(\"Before\")\n","print(\"train pos edges: \",len(train_pos_u),\"test pos edges: \",len(test_pos_u),\"val pos edges: \",len(val_pos_u))\n","\n","import random\n","random.seed(42)\n","lst=[]\n","#1:\n","for i in range(0,644):\n","    lst.append(i)\n","lst=random.sample(lst,129)\n","#2:\n","#back=[10,501,75,224,541,365,126,696,775,615,656,622,533,339,376,690,151,360,9,387,92,412,788,770] #some nodes that u don't want to keep in trainset (pos/neg)\n","#for i in back:\n","#  lst.append(i)\n","train_pos_u1=[]\n","train_pos_v1=[]\n","test_pos_u1=[]\n","test_pos_v1=[]\n","\n","for i in range(len(train_pos_u)):\n","  if train_pos_u[i] not in lst and train_pos_v[i] not in lst:\n","    train_pos_u1.append(train_pos_u[i])\n","    train_pos_v1.append(train_pos_v[i])\n","  if train_pos_u[i] in lst and train_pos_v[i] in lst:  \n","    test_pos_u1.append(train_pos_u[i])\n","    test_pos_v1.append(train_pos_v[i])\n","\n","\n","val_pos_u1=[]\n","val_pos_v1=[]\n","for i in range(len(val_pos_u)):\n","  if val_pos_u[i] not in lst and val_pos_v[i] not in lst:\n","    val_pos_u1.append(val_pos_u[i])\n","    val_pos_v1.append(val_pos_v[i])\n","  if val_pos_u[i] in lst and val_pos_v[i] in lst:\n","    test_pos_u1.append(val_pos_u[i])\n","    test_pos_v1.append(val_pos_v[i])\n","        \n","val_pos_u=torch.tensor(val_pos_u1)\n","val_pos_v=torch.tensor(val_pos_v1)\n","\n","\n","for i in range(len(test_pos_u)):\n","  if test_pos_u[i]  in lst and test_pos_v[i]  in lst:\n","    test_pos_u1.append(test_pos_u[i])\n","    test_pos_v1.append(test_pos_v[i]) \n","  if test_pos_u[i] not in lst and test_pos_v[i] not in lst:\n","    train_pos_u1.append(test_pos_u[i])\n","    train_pos_v1.append(test_pos_v[i])   \n","test_pos_u=torch.tensor(test_pos_u1)\n","test_pos_v=torch.tensor(test_pos_v1)\n","\n","train_pos_u=torch.tensor(train_pos_u1)\n","train_pos_v=torch.tensor(train_pos_v1)\n","\n","print(\"After\")\n","print(\"train pos edges: \",len(train_pos_u),\"test pos edges: \",len(test_pos_u),\"val pos edges: \",len(val_pos_u))\n","\n","'''\n","test_neg_u, test_neg_v = neg_u[test_neg_eids], neg_v[test_neg_eids]\n","train_neg_u, train_neg_v = neg_u[train_neg_eids], neg_v[train_neg_eids]\n","val_neg_u, val_neg_v = neg_u[val_neg_eids], neg_v[val_neg_eids]\n","'''\n","print(\"Before\")\n","print(\"train neg edges: \",len(train_neg_u),\"test neg edges: \",len(test_neg_u),\"val neg edges: \",len(val_neg_u))\n","\n","################\n","\n","train_neg_u1=[]\n","train_neg_v1=[]\n","test_neg_u1=[]\n","test_neg_v1=[]\n","for i in range(len(train_neg_u)):\n","  if train_neg_u[i] not in lst and train_neg_v[i] not in lst:\n","    if len(train_neg_u1)<= len(train_pos_u):\n","      train_neg_u1.append(train_neg_u[i])\n","      train_neg_v1.append(train_neg_v[i])\n","  if train_neg_u[i]  in lst and train_neg_v[i]  in lst:\n","    if len(test_neg_u1)<= len(test_pos_u):\n","      test_neg_u1.append(train_neg_u[i])\n","      test_neg_v1.append(train_neg_v[i])  \n","\n","\n","val_neg_u1=[]\n","val_neg_v1=[]\n","for i in range(len(val_pos_u)):\n","  if val_neg_u[i] not in lst and val_neg_v[i] not in lst:\n","    if len(val_neg_u1)<= len(val_pos_u):\n","      val_neg_u1.append(val_neg_u[i])\n","      val_neg_v1.append(val_neg_v[i])\n","  if val_neg_u[i]  in lst and val_neg_v[i]  in lst:\n","    if len(test_neg_u1)<= len(test_pos_u):\n","      test_neg_u1.append(val_neg_u[i])\n","      test_neg_v1.append(val_neg_v[i])  \n","val_neg_u=torch.tensor(val_neg_u1)\n","val_neg_v=torch.tensor(val_neg_v1)\n","\n","\n","for i in range(len(test_neg_u)):\n","  if test_neg_u[i]  in lst and test_neg_v[i]  in lst:\n","    if len(test_neg_u1)<=len(test_pos_u):\n","      test_neg_u1.append(int(test_neg_u[i]))\n","      test_neg_v1.append(int(test_neg_v[i]))\n","  if test_neg_u[i]  not in lst and test_neg_v[i]  not in lst:\n","    if len(train_neg_u1)<= len(train_pos_u):\n","      train_neg_u1.append(int(test_neg_u[i]))\n","      train_neg_v1.append(int(test_neg_v[i]))  \n","test_neg_u=torch.tensor(test_neg_u1)\n","test_neg_v=torch.tensor(test_neg_v1)\n","train_neg_u=torch.tensor(train_neg_u1)\n","train_neg_v=torch.tensor(train_neg_v1)\n","##################\n","print(\"After\")\n","print(\"train neg edges: \",len(train_neg_u),\"test neg edges: \",len(test_neg_u),\"val neg edges: \",len(val_neg_u))\n","\n","train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())# this is just the edge_list\n","train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n","test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n","test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n","val_pos_g = dgl.graph((val_pos_u, val_pos_v), num_nodes=g.number_of_nodes())\n","val_neg_g = dgl.graph((val_neg_u, val_neg_v), num_nodes=g.number_of_nodes())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6EfXcgXxa_M"},"outputs":[],"source":["p_num_edges=val_neg_g.num_edges()\n","u,v=val_pos_g.edges()\n","remain=len(u)-p_num_edges\n","\n","lst=range(0,remain)\n","val_pos_g.remove_edges(lst)\n","\n","\n","p_num_edges=train_neg_g.num_edges()\n","u,v=train_pos_g.edges()\n","remain=len(u)-p_num_edges\n","\n","lst=range(0,remain)\n","train_pos_g.remove_edges(lst)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OMYJY8Fe1bQO"},"outputs":[],"source":["#3: \n","l_u=[10,75,541,126,775,656]\n","l_v=[501,224,365,696,615,622]\n","test_pos_u1=[]\n","test_pos_v1=[]\n","\n","for i in range (len(l_u)):\n","  test_pos_u1.append(int(l_u[i]))\n","  test_pos_v1.append(int(l_v[i]))\n","test_pos_u=torch.tensor(test_pos_u1)\n","test_pos_v=torch.tensor(test_pos_v1)\n","  \n","l_u=[376,151,9,92.788]\n","l_v=[690,360,387,412,770]\n","test_neg_u1=[]\n","test_neg_v1=[]\n","\n","for i in range (len(l_u)):\n","  test_neg_u1.append(int(l_u[i]))\n","  test_neg_v1.append(int(l_v[i]))\n","test_neg_u=torch.tensor(test_neg_u1)\n","test_neg_v=torch.tensor(test_neg_v1)\n","\n","test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n","test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1657203816475,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"7NQ16ZlGNH47","outputId":"d68b6a8b-6d63-46a7-dc0a-b5fcb4052632"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of edges:  126946\n","num of train POSITIVE edges:  101558 , num of test POSITIVE edges:  12694 num of val POSITIVE edges:  12694\n","num of train NEGATIVE edges:  101558 , num of test NEGATIVE edges:  12694 num of val NEGATIVE edges:  12694\n"]}],"source":["print(\"Total number of edges: \",g.number_of_edges())\n","print(\"num of train POSITIVE edges: \",train_pos_g.number_of_edges(),\", num of test POSITIVE edges: \",test_pos_g.number_of_edges(),\"num of val POSITIVE edges: \",val_pos_g.number_of_edges())\n","print(\"num of train NEGATIVE edges: \",train_neg_g.number_of_edges(),\", num of test NEGATIVE edges: \",test_neg_g.number_of_edges(),\"num of val NEGATIVE edges: \",val_neg_g.number_of_edges())\n","\n","g_for_baseline = dgl.remove_edges(g, test_eids)\n","#dgl.save_graphs(\"drive/My Drive/Colab Notebooks/Hypergraph_my/a.bin\",g_for_baseline)\n","g_for_baseline = dgl.add_self_loop(g_for_baseline)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_U_zGixKUbG"},"outputs":[],"source":["'''\n","compute_loss() and compute_auc() define\n","'''\n","def compute_loss(pos_score, neg_score):\n","    scores = torch.cat([pos_score, neg_score])\n","    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n","    return F.binary_cross_entropy_with_logits(scores, labels)\n","\n","def compute_auc(pos_score, neg_score):\n","    scores = torch.cat([pos_score, neg_score]).numpy()\n","    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy() \n","    precision, recall, thresholds = precision_recall_curve(labels, scores)\n","    auc_precision_recall = auc(recall, precision)\n","    return roc_auc_score(labels, scores),auc(recall, precision)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBFa3jJD9Jo2"},"outputs":[],"source":["'''\n","Decoder part. \n","2 types of decoders.\n","Anyone u can use.\n","Source: DGL\n","'''\n","\n","import dgl.function as fn\n","class DotPredictor(nn.Module):\n","    def forward(self, g, h):\n","        with g.local_scope():\n","            g.ndata['h'] = h # here h is 822 drug features and g is the pos/neg train/test graph which is nothing but edge lis\n","            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n","            return g.edata['score'][:, 0]\n","\n","class MLPPredictor(nn.Module):\n","    def __init__(self, h_feats):\n","        super().__init__()\n","        self.W1 = nn.Linear(h_feats * 2, h_feats)\n","        self.W2 = nn.Linear(h_feats, 1)\n","\n","    def apply_edges(self, edges):\n","        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n","        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n","\n","    def forward(self, g, h):\n","        with g.local_scope():\n","            g.ndata['h'] = h\n","            g.apply_edges(self.apply_edges)\n","            return g.edata['score']\n","\n","\n","decoder = MLPPredictor(128)\n","#decoder = DotPredictor()# You can replace DotPredictor with MLPPredictor.\n","#opt = torch.optim.Adam(list(model.parameters()) + list(pred.parameters()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvzS_z3OGNdd"},"outputs":[],"source":["'''\n","Encoder part (HAT)\n","'''\n","class HyperAttn_ne(nn.Module):\n","    # edge attention  version\n","    def __init__(self, input_dim, query_dim, vertex_dim, edge_dim, dropout):\n","        super(HyperAttn_ne, self).__init__()\n","        self.dropout = dropout\n","        \n","        self.query_dim = query_dim\n","        self.vtx_lin_1layer = torch.nn.Linear(input_dim, vertex_dim)\n","        self.vtx_lin = torch.nn.Linear(vertex_dim, vertex_dim)\n","        \n","        self.qe_lin = torch.nn.Linear(edge_dim, query_dim)\n","        self.kv_lin = torch.nn.Linear(vertex_dim, query_dim)\n","        self.vv_lin = torch.nn.Linear(vertex_dim, edge_dim)\n","        \n","        self.qv_lin = torch.nn.Linear(vertex_dim, query_dim)\n","        self.ke_lin = torch.nn.Linear(edge_dim, query_dim)\n","        self.ve_lin = torch.nn.Linear(edge_dim, vertex_dim)\n","        \n","        #self.cls = nn.Linear(vertex_dim, num_class)\n","\n","    def attention(self, edges):\n","        attn_score = F.leaky_relu((edges.src['k'] * edges.dst['q']).sum(-1))\n","        return {'Attn': attn_score/np.sqrt(self.query_dim)}\n","    \n","    def message_func(self, edges):\n","        return {'v': edges.src['v'], 'Attn': edges.data['Attn']}\n","\n","    def reduce_func(self, nodes):\n","        attention_score = F.softmax((nodes.mailbox['Attn']), dim=1)\n","        aggr = torch.sum(attention_score.unsqueeze(-1) * nodes.mailbox['v'], dim=1)\n","        return {'h': aggr}\n","\n","    def forward(self, hyG, vfeat, efeat, first_layer, last_layer):\n","            if first_layer:\n","                feat_e = self.vtx_lin_1layer(efeat)\n","            else:\n","                feat_e = self.vtx_lin(efeat)        \n","            feat_v = vfeat\n","\n","            # edge attention\n","            hyG.ndata['h'] = {'edge': feat_e}\n","            hyG.ndata['k'] = {'edge' : self.ke_lin(feat_e)}\n","            hyG.ndata['v'] = {'edge' : self.ve_lin(feat_e)}\n","            hyG.ndata['q'] = {'node' : self.qv_lin(feat_v)}\n","            hyG.apply_edges(self.attention, etype='con')\n","            hyG.update_all(self.message_func, self.reduce_func, etype='con')\n","            feat_v = hyG.ndata['h']['node']\n","\n","            # node attention\n","            hyG.ndata['k'] = {'node' : self.kv_lin(feat_v)}\n","            hyG.ndata['v'] = {'node' : self.vv_lin(feat_v)}\n","            hyG.ndata['q'] = {'edge' : self.qe_lin(feat_e)}\n","            hyG.apply_edges(self.attention, etype='in')\n","            hyG.update_all(self.message_func, self.reduce_func, etype='in')\n","            feat_e = hyG.ndata['h']['edge']\n","            \n","            if not last_layer :\n","                feat_v = F.dropout(feat_v, self.dropout)\n","            if last_layer:\n","                #pred=self.cls(feat_v)\n","                return feat_v, feat_e\n","            else:\n","                return [hyG, feat_v, feat_e]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"17tBcT8M-BGG"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.gat1 = HyperAttn_ne(drug_X.shape[1],64,128,128,0.5)\n"," \n","    def forward(self,hyG, v_feat, e_feat,f,l):   \n","        h = self.gat1(hyG, v_feat, e_feat,f,l)\n","        return h\n","model = Model()  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jLSMx5T-ItX"},"outputs":[],"source":["# ----------- 3. set up loss and optimizer -------------- #\n","optimizer = torch.optim.Adam(itertools.chain(model.parameters(), decoder.parameters()), lr=0.005)\n","best_val_loss=1e10\n","patience=0\n","\n","for e in range(2000):\n","    # forward\n","    model.train()\n","    h=model(hyG, v_feat, e_feat,True,True)\n","    h=h[1]\n","    pos_score = decoder(train_pos_g, h)\n","    neg_score = decoder(train_neg_g, h)\n","    loss = compute_loss(pos_score, neg_score)\n","    \n","    # backward\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    with torch.no_grad():\n","      model.eval()\n","      pos_score = decoder(val_pos_g, h)\n","      neg_score = decoder(val_neg_g, h)\n","      #val_acc=compute_auc(pos_score, neg_score)\n","      val_loss = compute_loss(pos_score, neg_score)\n","      if val_loss<best_val_loss:\n","        best_val_loss=val_loss\n","        H=h\n","        E=e\n","        patience=0\n","        torch.save(decoder.state_dict(),'latest.pth')\n","      else:\n","        patience+=1  \n","    if patience>200:\n","      break\n","\n","    if e % 10 == 0:\n","      print('In epoch {}, train loss: {:.4f}, val loss: {:.4f} (best val loss: {:.4f})'.format(e, loss, val_loss, best_val_loss))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1657274291787,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"lcRFtgZoPWSb","outputId":"2c810038-8f5a-4090-9071-8fcdf2183c63"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Best Epoch: 934, Accuracy: 0.6843, Precision: 0.8296, Recall: 0.6478, F1-score 0.7275, ROC-AUC 0.7825, PR-AUC 0.8564\n"]}],"source":["decoder.load_state_dict(torch.load('latest.pth'))\n","with torch.no_grad():\n","  model.eval()\n","  pos_score = decoder(test_pos_g, H)\n","  neg_score = decoder(test_neg_g, H)\n","  test_acc=compute_auc(pos_score, neg_score)\n","\n","scores = torch.cat([pos_score, neg_score])\n","labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n","\n","m1 = tf.keras.metrics.BinaryAccuracy()\n","m1.update_state(labels,scores)\n","\n","sig_scores=F.sigmoid(scores)\n","m2 = tf.keras.metrics.Precision()\n","m2.update_state(labels,sig_scores)\n","M2=m2.result().numpy()\n","\n","m3 = tf.keras.metrics.Recall()\n","m3.update_state(labels,sig_scores)\n","M3=m3.result().numpy()\n","\n","F1=2*(M2*M3)/(M2+M3)\n","print(' Best Epoch: {}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score {:.4f}, ROC-AUC {:.4f}, PR-AUC {:.4f}'.format( E,m1.result().numpy(), M2, M3, F1,test_acc[0],test_acc[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W8DoKZu2Sb-F"},"outputs":[],"source":["##################################################### CASE STUDY #############################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHjhdEDQ9kac"},"outputs":[],"source":["lst=[]\n","for i in range (76760,76780): # range s ur choice \n","  if sig_scores[i]<=0.00001: # use a large threshold say 0.9\n","    #print(sig_scores[i])\n","    lst.append(i)\n","print(len(lst))\n","sig_scores[lst[0]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QMzBtsy3cThR"},"outputs":[],"source":["c=0\n","for i in lst:\n","  j=i-76560\n","  print(sig_scores[i],test_neg_u[j],test_neg_v[j])\n","  for m,n in D.items():\n","    if n==test_neg_u[j]:\n","      print(m)\n","      break\n","  c+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":127,"status":"ok","timestamp":1656838996711,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"DvmR9QgRxgW9","outputId":"ac88b3f3-3f1a-4918-e67f-304725abe0c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["1208\n"]}],"source":["for i,j in D.items():\n","  if i=='DB01165':\n","    print(j)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":211,"status":"ok","timestamp":1656888379757,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"oqzRrbrWcjTQ","outputId":"63e21f59-13a0-42e7-bd32-41431a326d7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0\n"]}],"source":["# where we are cross-checking between what we predicted and what is in the DrugBank (label)\n","for i in range (len(mat)):\n","  for j in range (len(mat)):\n","    if i==410    and j==1621:\n","      print(mat[i][j])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"erhG8Gyyxp9w"},"outputs":[],"source":["for i in range(len(test_neg_u)):\n","  if test_neg_u[i]==126 and test_neg_v[i]==338:\n","    print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltOucGU3U8fd"},"outputs":[],"source":["for i in range (len(test_pos_u)):\n","  if test_pos_u[i]==656 and test_pos_v[i]==622:\n","    print(sig_scores[i])\n","  if test_pos_u[i]==622 and test_pos_v[i]==656:\n","    print(sig_scores[i])  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3wFelXBH5rQ"},"outputs":[],"source":["********************************************************** GCN conv with ML **********************************"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75202,"status":"ok","timestamp":1657258679957,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"P7iePlsLFslt","outputId":"b30244c1-0ed2-498b-8528-93e8f6b2f69f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of edges:  126946\n","num of train POSITIVE edges:  114252 , num of test POSITIVE edges:  12694\n","num of train NEGATIVE edges:  114252 , num of test NEGATIVE edges:  12694\n"]}],"source":["'''\n","*************another approach\n","Train, test (both positive and negative) graphs creation for training and testing purpose from the dgl graph (g)\n","Source: DGL\n","'''\n","np.random.seed(42)\n","# Split edge set for training and testing\n","u, v = g.edges()\n","eids = np.arange(g.number_of_edges())\n","eids = np.random.permutation(eids)\n","eids=eids.tolist()\n","test_size = int(len(eids) * 0.1)\n","train_size=len(eids)-(test_size)\n","test_size1=int(test_size/2)\n","\n","\n","T1=[]\n","T2=[]\n","numba.njit(target=\"cuda\")\n","def func2(T1):\n","  for i in eids:\n","    m, n = int(u[i]), int(v[i])\n","    c=(m,n)\n","    T1.append(c)\n","  return T1  \n","function1=jit(parallel=True) (func2)\n","T1 = function1(T1)\n","\n","\n","numba.njit(target=\"cuda\")\n","def func3(T1,T2):\n","  for i in T1:\n","    m=i[0]\n","    n=i[1]\n","    c=(m,n)\n","    d=(n,m)\n","    if c not in T2 and d not in T2:\n","      T2.append(c)\n","    if len(T2)>=test_size1:\n","      break  \n","  return T2\n","function1=jit(parallel=True) (func3)\n","T2 = function1(T1,T2)\n","\n","\n","test_eids=[]\n","numba.njit(target=\"cuda\")\n","def func4(T2,test_eids,g):\n","  for i in T2:\n","    m= i[0]\n","    n=i[1]\n","    c=g.edge_id(m,n)\n","    d=g.edge_id(n,m)\n","    test_eids.append(c)\n","    test_eids.append(d)\n","  return test_eids  \n","function1=jit(parallel=True) (func4)\n","test_eids = function1(T2,test_eids,g)\n","\n","\n","train_eids=[]\n","numba.njit(target=\"cuda\")\n","def func6(eids,test_eids,train_eids):\n","  i=0\n","  for j in eids:\n","    a=int(eids[i])\n","    if a not in test_eids  and len(train_eids)<train_size:\n","      train_eids.append(a)  \n","    i=i+1\n","  return train_eids    \n","function1=jit(parallel=True) (func6)\n","train_eids = function1(eids,test_eids,train_eids)     \n","\n","# main one. splitting into train and test set.\n","test_pos_u, test_pos_v = u[test_eids], v[test_eids]\n","train_pos_u, train_pos_v = u[train_eids], v[train_eids]\n","\n","\n","\n","# Find all negative edges and split them for training and testing\n","adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n","adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n","neg_u, neg_v = np.where(adj_neg != 0)\n","\n","neg_eids = np.random.choice(len(neg_u), g.number_of_edges(),replace=False)\n","g_=nx.from_numpy_matrix(adj_neg)\n","g_=dgl.from_networkx(g_)\n","\n","T1=[]\n","T2=[]\n","numba.njit(target=\"cuda\")\n","def func2(T1):\n","  for i in neg_eids:\n","    m, n = int(neg_u[i]), int(neg_v[i])\n","    c=(m,n)\n","    T1.append(c)\n","  return T1  \n","function1=jit(parallel=True) (func2)\n","T1 = function1(T1)\n","\n","\n","numba.njit(target=\"cuda\")\n","def func3(T1,T2):\n","  for i in T1:\n","    m=i[0]\n","    n=i[1]\n","    c=(m,n)\n","    d=(n,m)\n","    if c not in T2 and d not in T2:\n","      T2.append(c)\n","    if len(T2)>=test_size1:\n","      break  \n","  return T2\n","function1=jit(parallel=True) (func3)\n","T2 = function1(T1,T2)\n","\n","\n","test_neg_eids=[]\n","numba.njit(target=\"cuda\")\n","def func4(T2,test_eids,g):\n","  for i in T2:\n","    m= i[0]\n","    n=i[1]\n","    c=g.edge_id(m,n)\n","    d=g.edge_id(n,m)\n","    test_neg_eids.append(c)\n","    test_neg_eids.append(d)\n","  return test_neg_eids  \n","function1=jit(parallel=True) (func4)\n","test_neg_eids = function1(T2,test_neg_eids,g_)\n","\n","\n","train_neg_eids=[]\n","numba.njit(target=\"cuda\")\n","def func6(neg_eids,test_neg_eids,train_neg_eids):\n","  i=0\n","  for j in neg_eids:\n","    a=int(neg_eids[i])\n","    if a not in test_neg_eids and len(train_neg_eids)<(train_size):\n","      train_neg_eids.append(a)  \n","    i=i+1\n","  return train_neg_eids    \n","function1=jit(parallel=True) (func6)\n","train_neg_eids = function1(neg_eids,test_neg_eids,train_neg_eids)     \n","\n","test_neg_u, test_neg_v = neg_u[test_neg_eids], neg_v[test_neg_eids]\n","train_neg_u, train_neg_v = neg_u[train_neg_eids], neg_v[train_neg_eids]\n","\n","\n","train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())# this is just the edge_list\n","train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n","test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n","test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n","\n","print(\"Total number of edges: \",g.number_of_edges())\n","print(\"num of train POSITIVE edges: \",train_pos_g.number_of_edges(),\", num of test POSITIVE edges: \",test_pos_g.number_of_edges())\n","print(\"num of train NEGATIVE edges: \",train_neg_g.number_of_edges(),\", num of test NEGATIVE edges: \",test_neg_g.number_of_edges())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLxNd69UMQCc"},"outputs":[],"source":["g_for_baseline = dgl.remove_edges(g, test_eids)\n","#dgl.save_graphs(\"drive/My Drive/Colab Notebooks/Hypergraph_my/a.bin\",g_for_baseline)\n","g_for_baseline = dgl.add_self_loop(g_for_baseline)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjwnjV_2H-a_"},"outputs":[],"source":["class MLPPredictor(nn.Module):\n","    def __init__(self, h_feats):\n","        super().__init__()\n","    def apply_edges(self, edges):\n","        h = torch.cat([edges.src['h'], edges.dst['h']], 1).squeeze(1)\n","        return {'score': h.squeeze(1)}\n","\n","    def forward(self, g, h):\n","        with g.local_scope():\n","            g.ndata['h'] = h\n","            g.apply_edges(self.apply_edges)\n","            return g.edata['score']\n","decoder = MLPPredictor(128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VE7rYycwkTL"},"outputs":[],"source":["from dgl.nn import GraphConv\n","\n","# ----------- 2. create model -------------- #\n","# build a two-layer GraphSAGE model\n","class GCN(nn.Module):\n","    def __init__(self, in_feats, h_feats):\n","        super(GCN, self).__init__()\n","        self.conv1 = GraphConv(in_feats, h_feats)\n","        self.conv2 = GraphConv(h_feats, h_feats)\n","\n","    def forward(self, g, in_feat):\n","        h = self.conv1(g, in_feat)\n","        h = F.relu(h)\n","        h = self.conv2(g, h)\n","        return h\n","\n","model = GCN(645, 128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykcPsc-i2d9z"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from keras.utils import np_utils\n","from sklearn.model_selection import cross_validate\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegressionCV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1657257917495,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"2xDCy7Tt0vnC","outputId":"89c5b768-8a8d-45d4-cba8-34090c952887"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 1., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 1.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 1., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 1., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 1.]])"]},"metadata":{},"execution_count":139}],"source":["e_feat = torch.zeros(645, 645)\n","e_feat.fill_diagonal_(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yEfDaXuUOQ8"},"outputs":[],"source":["'''\n","when u want to use saved GCN embeddings\n","'''\n","h =np.loadtxt(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/afterTrainingEmbedsGCN.txt\")\n","h=torch.tensor(h)\n","h=h.double()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imlq53oXzqRE"},"outputs":[],"source":["h = model(g_for_baseline, e_feat)    #uncomment it if u want to use original GCN from this notebook\n","pos_score_train = decoder(train_pos_g, h)\n","neg_score_train = decoder(train_neg_g, h)\n","pos_score_test = decoder(test_pos_g, h)\n","neg_score_test = decoder(test_neg_g, h)\n","\n","scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n","labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n","scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n","labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjDQloLNth98"},"outputs":[],"source":["modelNN = Sequential()\n","modelNN.add(Dense(10, input_dim=256, activation='sigmoid'))\n","modelNN.add(Dense(1, activation='sigmoid'))\n","modelNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","modelNN.fit(scores_train, labels_train, epochs=50, batch_size=20)\n","prediction_NN = modelNN.predict(scores_test)\n","\n","print ('Accuracy:', accuracy_score(labels_test, prediction_NN.round()))\n","print ('F1 score:', f1_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Recall:', recall_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Precision:', precision_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('ROC-AUC:', roc_auc_score(labels_test, prediction_NN.round()))\n","precision, recall, thresholds = precision_recall_curve(labels_test, prediction_NN)\n","auc_precision_recall = auc(recall, precision)\n","print(\"PR-AUC: \",auc_precision_recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LN00S7FpODnJ"},"outputs":[],"source":["print(\"LR\")\n","lr = LogisticRegression(class_weight=\"balanced\")\n","lr.fit(scores_train, labels_train)\n","predictions = lr.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"NB\")\n","gnb = GaussianNB()\n","gnb.fit(scores_train, labels_train)\n","predictions = gnb.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"RF\")\n","clf = RandomForestClassifier(max_depth=2, random_state=0)\n","clf.fit(scores_train, labels_train)\n","predictions = clf.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gGnogCKReMV"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oivgp2CGWg7I"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0o6fss5WhhG"},"outputs":[],"source":["********************************************************* Graph SAGE with ML ********************************************8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pf96mSJ4WhQd"},"outputs":[],"source":["from dgl.nn import SAGEConv\n","\n","# ----------- 2. create model -------------- #\n","# build a two-layer GraphSAGE model\n","class GraphSAGE(nn.Module):\n","    def __init__(self, in_feats, h_feats):\n","        super(GraphSAGE, self).__init__()\n","        self.conv1 = SAGEConv(in_feats, h_feats, 'pool')\n","        self.conv2 = SAGEConv(h_feats, h_feats, 'pool')\n","\n","    def forward(self, g, in_feat):\n","        h = self.conv1(g, in_feat)\n","        h = F.relu(h)\n","        h = self.conv2(g, h)\n","        return h\n","\n","model = GraphSAGE(645, 128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9Paxz4Ng-1l"},"outputs":[],"source":["'''\n","when u want to use saved GCN embeddings\n","'''\n","h =np.loadtxt(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/afterTrainingEmbedsGS.txt\")\n","h=torch.tensor(h)\n","h=h.double()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9wKMe8-XRur"},"outputs":[],"source":["h = model(g_for_baseline, e_feat)    #uncomment it if u want to use original GCN from this notebook\n","pos_score_train = decoder(train_pos_g, h)\n","neg_score_train = decoder(train_neg_g, h)\n","pos_score_test = decoder(test_pos_g, h)\n","neg_score_test = decoder(test_neg_g, h)\n","\n","scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n","labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n","scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n","labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i1lEFDulnGZV"},"outputs":[],"source":["modelNN = Sequential()\n","modelNN.add(Dense(2, input_dim=200, activation='relu'))\n","modelNN.add(Dense(1, activation='sigmoid'))\n","modelNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","modelNN.fit(scores_train, labels_train, epochs=30, batch_size=10)\n","prediction_NN = modelNN.predict(scores_test)\n","\n","print ('Accuracy:', accuracy_score(labels_test, prediction_NN.round()))\n","print ('F1 score:', f1_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Recall:', recall_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Precision:', precision_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('ROC-AUC:', roc_auc_score(labels_test, prediction_NN.round()))\n","precision, recall, thresholds = precision_recall_curve(labels_test, prediction_NN)\n","auc_precision_recall = auc(recall, precision)\n","print(\"PR-AUC: \",auc_precision_recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kNGAxn1vXU8b","executionInfo":{"status":"ok","timestamp":1657258764935,"user_tz":300,"elapsed":43992,"user":{"displayName":"kms hamim","userId":"17071813694682210730"}},"outputId":"4b10959a-cd28-4469-fe48-be51fc111436"},"outputs":[{"output_type":"stream","name":"stdout","text":["LR\n","accuracy:  0.8580431700015756 precision:  0.8583509195434026 recall:  0.8580431700015756 f1-score:  0.8580126855458469 ROC-AUC:  0.8580431700015756 PR-AUC:  0.8920986304795103\n","NB\n","accuracy:  0.8037655585315897 precision:  0.8205412390813573 recall:  0.8037655585315897 f1-score:  0.8011640141958215 ROC-AUC:  0.8037655585315898 PR-AUC:  0.8531431672682562\n","RF\n","accuracy:  0.8103434693556011 precision:  0.8138989935092009 recall:  0.8103434693556011 f1-score:  0.8098048858172999 ROC-AUC:  0.8103434693556011 PR-AUC:  0.8561350355827504\n"]}],"source":["print(\"LR\")\n","lr = LogisticRegression(class_weight=\"balanced\")\n","lr.fit(scores_train, labels_train)\n","predictions = lr.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"NB\")\n","gnb = GaussianNB()\n","gnb.fit(scores_train, labels_train)\n","predictions = gnb.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"RF\")\n","clf = RandomForestClassifier(max_depth=2, random_state=0)\n","clf.fit(scores_train, labels_train)\n","predictions = clf.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxLv9xd4bFV8"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WHBGPmHbGF8"},"outputs":[],"source":["******************************************************************** GAT with ML ******************************************"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EaJeGFxobFln"},"outputs":[],"source":["from dgl.nn import GATConv\n","\n","# ----------- 2. create model -------------- #\n","# build a two-layer GraphSAGE model\n","class GAT(nn.Module):\n","    def __init__(self, in_feats, h_feats):\n","        super(GAT, self).__init__()\n","        self.conv1 = GATConv(in_feats, h_feats, num_heads=1)\n","        self.conv2 = GATConv(h_feats, h_feats,num_heads=1)\n","\n","    def forward(self, g, in_feat):\n","        h = self.conv1(g, in_feat)\n","        h = F.relu(h)\n","        h = self.conv2(g, h)\n","        return h\n","\n","model = GAT(LEN, 128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ot_LgJyXhEgN"},"outputs":[],"source":["'''\n","when u want to use saved GCN embeddings\n","'''\n","h =np.loadtxt(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/afterTrainingEmbedsGAT.txt\")\n","h=torch.tensor(h)\n","h=h.double()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Wg1jBydbl1S"},"outputs":[],"source":["#h = model(g_for_baseline, e_feat)    #uncomment it if u want to use original GCN from this notebook\n","pos_score_train = decoder(train_pos_g, h)\n","neg_score_train = decoder(train_neg_g, h)\n","pos_score_test = decoder(test_pos_g, h)\n","neg_score_test = decoder(test_neg_g, h)\n","\n","scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n","labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n","scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n","labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmzBHcCMnISL"},"outputs":[],"source":["modelNN = Sequential()\n","modelNN.add(Dense(2, input_dim=200, activation='relu'))\n","modelNN.add(Dense(1, activation='sigmoid'))\n","modelNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","modelNN.fit(scores_train, labels_train, epochs=30, batch_size=10)\n","prediction_NN = modelNN.predict(scores_test)\n","\n","print ('Accuracy:', accuracy_score(labels_test, prediction_NN.round()))\n","print ('F1 score:', f1_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Recall:', recall_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Precision:', precision_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('ROC-AUC:', roc_auc_score(labels_test, prediction_NN.round()))\n","precision, recall, thresholds = precision_recall_curve(labels_test, prediction_NN)\n","auc_precision_recall = auc(recall, precision)\n","print(\"PR-AUC: \",auc_precision_recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29664,"status":"ok","timestamp":1657228569297,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"qcxRNSgibf08","outputId":"a1e87406-c043-4ee0-e90d-2a0763c4bc2c"},"outputs":[{"name":"stdout","output_type":"stream","text":["LR\n","accuracy:  0.5437608318890814 precision:  0.5442210584531123 recall:  0.5437608318890814 f1-score:  0.5425706685547892 ROC-AUC:  0.5437608318890814 PR-AUC:  0.6685471815545543\n","NB\n","accuracy:  0.5205215062234126 precision:  0.5994697263323433 recall:  0.5205215062234126 f1-score:  0.40183122203828264 ROC-AUC:  0.5205215062234126 PR-AUC:  0.7469184485178153\n","RF\n","accuracy:  0.6097368835670396 precision:  0.6097372131784978 recall:  0.6097368835670396 f1-score:  0.6097365905140091 ROC-AUC:  0.6097368835670396 PR-AUC:  0.7074243723569373\n"]}],"source":["print(\"LR\")\n","lr = LogisticRegression(class_weight=\"balanced\")\n","lr.fit(scores_train, labels_train)\n","predictions = lr.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"NB\")\n","gnb = GaussianNB()\n","gnb.fit(scores_train, labels_train)\n","predictions = gnb.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"RF\")\n","clf = RandomForestClassifier(max_depth=2, random_state=0)\n","clf.fit(scores_train, labels_train)\n","predictions = clf.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbQB0DiBxK9C"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6HO936dD0LHc"},"outputs":[],"source":["************************************************************************ GIN with ML ***************************************************888888"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z39HV4nzxJdj"},"outputs":[],"source":["from dgl.nn import GINConv\n","lin1 = torch.nn.Linear(824, 100)\n","lin2 = torch.nn.Linear(100, 100)\n","# ----------- 2. create model -------------- #\n","# build a two-layer GraphSAGE model\n","class GIN(nn.Module):\n","    def __init__(self, in_feats, h_feats):\n","        super(GIN, self).__init__()\n","        self.conv1 = GINConv(lin1, 'max')\n","        self.conv2 = GINConv(lin2, 'max')\n","        \n","        \n","    def forward(self, g, in_feat):\n","        h = self.conv1(g, in_feat)\n","        h = F.relu(h)\n","        h = self.conv2(g, h)\n","        return h\n","\n","model = GIN(824, 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LXoETk8sn8CL"},"outputs":[],"source":["h = model(g_for_baseline, e_feat)\n","pos_score_train = decoder(train_pos_g, h)\n","neg_score_train = decoder(train_neg_g, h)\n","pos_score_test = decoder(test_pos_g, h)\n","neg_score_test = decoder(test_neg_g, h)\n","\n","\n","scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n","labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n","scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n","labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j36axVvPnKHm"},"outputs":[],"source":["modelNN = Sequential()\n","modelNN.add(Dense(2, input_dim=200, activation='relu'))\n","modelNN.add(Dense(1, activation='sigmoid'))\n","modelNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","modelNN.fit(scores_train, labels_train, epochs=30, batch_size=10)\n","prediction_NN = modelNN.predict(scores_test)\n","\n","print ('Accuracy:', accuracy_score(labels_test, prediction_NN.round()))\n","print ('F1 score:', f1_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Recall:', recall_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Precision:', precision_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('ROC-AUC:', roc_auc_score(labels_test, prediction_NN.round()))\n","precision, recall, thresholds = precision_recall_curve(labels_test, prediction_NN)\n","auc_precision_recall = auc(recall, precision)\n","print(\"PR-AUC: \",auc_precision_recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-c49mdIyyYPh"},"outputs":[],"source":["print(\"LR\")\n","lr = LogisticRegression(class_weight=\"balanced\")\n","lr.fit(scores_train, labels_train)\n","predictions = lr.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"NB\")\n","gnb = GaussianNB()\n","gnb.fit(scores_train, labels_train)\n","predictions = gnb.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"RF\")\n","clf = RandomForestClassifier(max_depth=2, random_state=0)\n","clf.fit(scores_train, labels_train)\n","predictions = clf.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlEgtbFQ0QlA"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4PG3Ygjt0P8d"},"outputs":[],"source":["************************************************************************ AGNNConv with ML ***************************************************888888"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HtMhAQIU0PM2"},"outputs":[],"source":["from dgl.nn import AGNNConv\n","# ----------- 2. create model -------------- #\n","# build a two-layer GraphSAGE model\n","class AGNN(nn.Module):\n","    def __init__(self):\n","        super(AGNN, self).__init__()\n","        self.conv1 = AGNNConv()\n","        self.conv2 = AGNNConv()\n","          \n","    def forward(self, g, in_feat):\n","        h = self.conv1(g, in_feat)\n","        h = F.relu(h)\n","        h = self.conv2(g, h)\n","        return h\n","\n","model = AGNN()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0s_LHhuyISA"},"outputs":[],"source":["h = model(g_for_baseline, e_feat)\n","pos_score_train = decoder(train_pos_g, h)\n","neg_score_train = decoder(train_neg_g, h)\n","pos_score_test = decoder(test_pos_g, h)\n","neg_score_test = decoder(test_neg_g, h)\n","\n","scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n","labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n","scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n","labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MGTo36xKnMDe"},"outputs":[],"source":["modelNN = Sequential()\n","modelNN.add(Dense(2, input_dim=1648, activation='relu'))\n","modelNN.add(Dense(1, activation='sigmoid'))\n","modelNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","modelNN.fit(scores_train, labels_train, epochs=30, batch_size=10)\n","prediction_NN = modelNN.predict(scores_test)\n","\n","print ('Accuracy:', accuracy_score(labels_test, prediction_NN.round()))\n","print ('F1 score:', f1_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Recall:', recall_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Precision:', precision_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('ROC-AUC:', roc_auc_score(labels_test, prediction_NN.round()))\n","precision, recall, thresholds = precision_recall_curve(labels_test, prediction_NN)\n","auc_precision_recall = auc(recall, precision)\n","print(\"PR-AUC: \",auc_precision_recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qPAlFTQ0ygV"},"outputs":[],"source":["print(\"LR\")\n","lr = LogisticRegression(class_weight=\"balanced\")\n","lr.fit(scores_train, labels_train)\n","predictions = lr.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"NB\")\n","gnb = GaussianNB()\n","gnb.fit(scores_train, labels_train)\n","predictions = gnb.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"RF\")\n","clf = RandomForestClassifier(max_depth=2, random_state=0)\n","clf.fit(scores_train, labels_train)\n","predictions = clf.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O4mDm8PU003B"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MyUzM0VWzQjC"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KXZ_MHWbzRCw"},"outputs":[],"source":["***************************************************************** DeepWalk **********************************"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p0jc4LGYt5UI"},"outputs":[],"source":["g_for_baseline_adj_matrix=g_for_baseline.adjacency_matrix_scipy()\n","G_baseline = nx.from_numpy_array(g_for_baseline_adj_matrix,create_using=nx.DiGraph)\n","nx.write_edgelist(G_baseline, \"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/G_baseline.txt\", data=False)\n","\n","# train model and generate embedding\n","model = DeepWalk(walk_length=100, dimensions=128, window_size=5)\n","model.fit(G_baseline)\n","h = model.get_embedding()\n","h=torch.from_numpy(h)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TXqloGE1n7tQ"},"outputs":[],"source":["pos_score_train = decoder(train_pos_g, h)\n","neg_score_train = decoder(train_neg_g, h)\n","pos_score_test = decoder(test_pos_g, h)\n","neg_score_test = decoder(test_neg_g, h)\n","\n","scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n","labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n","scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n","labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuUB9mLqnOLd"},"outputs":[],"source":["modelNN = Sequential()\n","modelNN.add(Dense(2, input_dim=200, activation='relu'))\n","modelNN.add(Dense(1, activation='sigmoid'))\n","modelNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","modelNN.fit(scores_train, labels_train, epochs=30, batch_size=10)\n","prediction_NN = modelNN.predict(scores_test)\n","\n","print ('Accuracy:', accuracy_score(labels_test, prediction_NN.round()))\n","print ('F1 score:', f1_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Recall:', recall_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Precision:', precision_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('ROC-AUC:', roc_auc_score(labels_test, prediction_NN.round()))\n","precision, recall, thresholds = precision_recall_curve(labels_test, prediction_NN)\n","auc_precision_recall = auc(recall, precision)\n","print(\"PR-AUC: \",auc_precision_recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50708,"status":"ok","timestamp":1657211558907,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"w2l0fnbN0oDw","outputId":"7e7060b9-3fd7-479e-c043-db0b06bea378"},"outputs":[{"name":"stdout","output_type":"stream","text":["LR\n","accuracy:  0.803608003781314 precision:  0.8040037464734512 recall:  0.803608003781314 f1-score:  0.8035440687146947 ROC-AUC:  0.803608003781314 PR-AUC:  0.8519296455746475\n","NB\n","accuracy:  0.8127461792973059 precision:  0.8129760221290243 recall:  0.8127461792973059 f1-score:  0.8127117941938743 ROC-AUC:  0.8127461792973059 PR-AUC:  0.8588212491450073\n","RF\n","accuracy:  0.8079801481014652 precision:  0.8085828471645349 recall:  0.8079801481014652 f1-score:  0.8078863428865861 ROC-AUC:  0.8079801481014655 PR-AUC:  0.8549919522864071\n"]}],"source":["print(\"LR\")\n","lr = LogisticRegression(class_weight=\"balanced\")\n","lr.fit(scores_train, labels_train)\n","predictions = lr.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"NB\")\n","gnb = GaussianNB()\n","gnb.fit(scores_train, labels_train)\n","predictions = gnb.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"RF\")\n","clf = RandomForestClassifier(max_depth=2, random_state=0)\n","clf.fit(scores_train, labels_train)\n","predictions = clf.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nwHYtgNexkhO"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dStwQAqB4WX3"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p0ENKLDg4R72"},"outputs":[],"source":["******************************************************************* Node2Vec ***********************************"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nd4bWCa9ktau"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8b2TBKC4YWT"},"outputs":[],"source":["h=np.loadtxt('/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/node_2vec_embeddings.txt',delimiter=',')\n","h=torch.from_numpy(h)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVfHtc63XuZa"},"outputs":[],"source":["pos_score_train = decoder(train_pos_g, h)\n","neg_score_train = decoder(train_neg_g, h)\n","pos_score_test = decoder(test_pos_g, h)\n","neg_score_test = decoder(test_neg_g, h)\n","\n","scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n","labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n","scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n","labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vDksPeX5nQi3"},"outputs":[],"source":["modelNN = Sequential()\n","modelNN.add(Dense(2, input_dim=200, activation='relu'))\n","modelNN.add(Dense(1, activation='sigmoid'))\n","modelNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","modelNN.fit(scores_train, labels_train, epochs=30, batch_size=10)\n","prediction_NN = modelNN.predict(scores_test)\n","\n","print ('Accuracy:', accuracy_score(labels_test, prediction_NN.round()))\n","print ('F1 score:', f1_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Recall:', recall_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('Precision:', precision_score(labels_test, prediction_NN.round(),average='weighted'))\n","print ('ROC-AUC:', roc_auc_score(labels_test, prediction_NN.round()))\n","precision, recall, thresholds = precision_recall_curve(labels_test, prediction_NN)\n","auc_precision_recall = auc(recall, precision)\n","print(\"PR-AUC: \",auc_precision_recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42967,"status":"ok","timestamp":1657258821524,"user":{"displayName":"kms hamim","userId":"17071813694682210730"},"user_tz":300},"id":"c6xQI7lWXw7M","outputId":"d6c47053-122e-4080-f5c7-b969d7d46eec"},"outputs":[{"output_type":"stream","name":"stdout","text":["LR\n","accuracy:  0.8432724121632267 precision:  0.8435291119502388 recall:  0.8432724121632267 f1-score:  0.8432431282872891 ROC-AUC:  0.8432724121632268 PR-AUC:  0.8813043148436112\n","NB\n","accuracy:  0.8139672286119426 precision:  0.8140282832006075 recall:  0.8139672286119426 f1-score:  0.8139581858703779 ROC-AUC:  0.8139672286119426 PR-AUC:  0.8600595565157555\n","RF\n","accuracy:  0.7811958405545927 precision:  0.7820122582432303 recall:  0.7811958405545927 f1-score:  0.7810373678634227 ROC-AUC:  0.7811958405545928 PR-AUC:  0.8354438803276918\n"]}],"source":["print(\"LR\")\n","lr = LogisticRegression(class_weight=\"balanced\")\n","lr.fit(scores_train, labels_train)\n","predictions = lr.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"NB\")\n","gnb = GaussianNB()\n","gnb.fit(scores_train, labels_train)\n","predictions = gnb.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"RF\")\n","clf = RandomForestClassifier(max_depth=2, random_state=0)\n","clf.fit(scores_train, labels_train)\n","predictions = clf.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"szIJQZdhYRnH"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHv6mJ8XYRKC"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"or5cEcOsn7Ce"},"outputs":[],"source":["************************************************************************ Unsupervised GNNs ******************************************************************"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y64nRzaNnbc7"},"outputs":[],"source":["import sys\n","import math\n","import copy\n","import random\n","from collections import defaultdict\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from scipy.sparse import csr_matrix\n","\n","class GNN(object):\n","    \"\"\"Graph Neural Networks that can be easily called and used.\n","    Authors of this code package:\n","    Tong Zhao, tzhao2@nd.edu\n","    Tianwen Jiang, twjiang@ir.hit.edu.cn\n","    Last updated: 11/25/2019\n","    Parameters\n","    ----------\n","    adj_matrix: scipy.sparse.csr_matrix\n","        The adjacency matrix of the graph, where nonzero entries indicates edges.\n","        The number of each nonzero entry indicates the number of edges between these two nodes.\n","    features: numpy.ndarray, optional\n","        The 2-dimension np array that stores given raw feature of each node, where the i-th row\n","        is the raw feature vector of node i.\n","        When raw features are not given, one-hot degree features will be used.\n","    labels: list or 1-D numpy.ndarray, optional\n","        The class label of each node. Used for supervised learning.\n","    supervised: bool, optional, default False\n","        Whether to use supervised learning.\n","    model: {'gat', 'graphsage'}, default 'gat'\n","        The GNN model to be used.\n","        - 'graphsage' is GraphSAGE: https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n","        - 'gat' is graph attention network: https://arxiv.org/pdf/1710.10903.pdf\n","    n_layer: int, optional, default 2\n","        Number of layers in the GNN\n","    emb_size: int, optional, default 128\n","        Size of the node embeddings to be learnt\n","    random_state, int, optional, default 1234\n","        Random seed\n","    device: {'cpu', 'cuda', 'auto'}, default 'auto'\n","        The device to use.\n","    epochs: int, optional, default 5\n","        Number of epochs for training\n","    batch_size: int, optional, default 20\n","        Number of node per batch for training\n","    lr: float, optional, default 0.7\n","        Learning rate\n","    unsup_loss_type: {'margin', 'normal'}, default 'margin'\n","        Loss function to be used for unsupervised learning\n","        - 'margin' is a hinge loss with margin of 3\n","        - 'normal' is the unsupervised loss function described in the paper of GraphSAGE\n","    print_progress: bool, optional, default True\n","        Whether to print the training progress\n","    \"\"\"\n","    def __init__(self, adj_matrix, features=None, labels=None, supervised=False, model='gat', n_layer=2, emb_size=128, random_state=1234, device='auto', epochs=5, batch_size=20, lr=0.7, unsup_loss_type='margin', print_progress=True):\n","        super(GNN, self).__init__()\n","        # fix random seeds\n","        random.seed(random_state)\n","        np.random.seed(random_state)\n","        torch.manual_seed(random_state)\n","        torch.cuda.manual_seed_all(random_state)\n","        # set parameters\n","        self.supervised = supervised\n","        self.lr = lr\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.unsup_loss_type = unsup_loss_type\n","        self.print_progress = print_progress\n","        self.gat = False\n","        self.gcn = False\n","        if model == 'gat':\n","            self.gat = True\n","            self.model_name = 'GAT'\n","        elif model == 'gcn':\n","            self.gcn = True\n","            self.model_name = 'GCN'\n","        else:\n","            self.model_name = 'GraphSAGE'\n","        # set device\n","        if device == 'auto':\n","            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        else:\n","            self.device = device\n","\n","        # load data\n","        self.dl = DataLoader(adj_matrix, features, labels, supervised, self.device)\n","\n","        self.gnn = GNN_model(n_layer, emb_size, self.dl, self.device, gat=self.gat, gcn=self.gcn)\n","        self.gnn.to(self.device)\n","\n","        if supervised:\n","            n_classes = len(set(labels))\n","            self.classification = Classification(emb_size, n_classes)\n","            self.classification.to(self.device)\n","\n","    def fit(self):\n","        train_nodes = copy.deepcopy(self.dl.nodes_train)\n","\n","        if self.supervised:\n","            labels = self.dl.labels\n","            models = [self.gnn, self.classification]\n","        else:\n","            unsup_loss = Unsup_Loss(self.dl, self.device)\n","            models = [self.gnn]\n","            if self.unsup_loss_type == 'margin':\n","                num_neg = 6\n","            elif self.unsup_loss_type == 'normal':\n","                num_neg = 100\n","\n","        for epoch in range(self.epochs):\n","            np.random.shuffle(train_nodes)\n","\n","            params = []\n","            for model in models:\n","                for param in model.parameters():\n","                    if param.requires_grad:\n","                        params.append(param)\n","            optimizer = torch.optim.SGD(params, lr=self.lr)\n","            optimizer.zero_grad()\n","            for model in models:\n","                model.zero_grad()\n","\n","            batches = math.ceil(len(train_nodes) / self.batch_size)\n","            visited_nodes = set()\n","            if self.print_progress:\n","                tqdm_bar = tqdm(range(batches), ascii=True, leave=False)\n","            else:\n","                tqdm_bar = range(batches)\n","            for index in tqdm_bar:\n","                if not self.supervised and len(visited_nodes) == len(train_nodes):\n","                    # finish this epoch if all nodes are visited\n","                    if self.print_progress:\n","                        tqdm_bar.close()\n","                    break\n","                nodes_batch = train_nodes[index*self.batch_size:(index+1)*self.batch_size]\n","                # extend nodes batch for unspervised learning\n","                if not self.supervised:\n","                    nodes_batch = np.asarray(list(unsup_loss.extend_nodes(nodes_batch, num_neg=num_neg)))\n","                visited_nodes |= set(nodes_batch)\n","                # feed nodes batch to the GNN and returning the nodes embeddings\n","                embs_batch = self.gnn(nodes_batch)\n","                # calculate loss\n","                if self.supervised:\n","                    # superivsed learning\n","                    logists = self.classification(embs_batch)\n","                    labels_batch = labels[nodes_batch]\n","                    loss_sup = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n","                    loss_sup /= len(nodes_batch)\n","                    loss = loss_sup\n","                else:\n","                    # unsupervised learning\n","                    if self.unsup_loss_type == 'margin':\n","                        loss_net = unsup_loss.get_loss_margin(embs_batch, nodes_batch)\n","                    elif self.unsup_loss_type == 'normal':\n","                        loss_net = unsup_loss.get_loss_sage(embs_batch, nodes_batch)\n","                    loss = loss_net\n","\n","                if self.print_progress:\n","                    progress_message = '{} Epoch: [{}/{}], current loss: {:.4f}, touched nodes [{}/{}] '.format(\n","                                    self.model_name, epoch+1, self.epochs, loss.item(), len(visited_nodes), len(train_nodes))\n","                    tqdm_bar.set_description(progress_message)\n","\n","                loss.backward()\n","                for model in models:\n","                    nn.utils.clip_grad_norm_(model.parameters(), 5)\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                for model in models:\n","                    model.zero_grad()\n","\n","    def generate_embeddings(self):\n","        nodes = self.dl.nodes_train\n","        b_sz = 500\n","        batches = math.ceil(len(nodes) / b_sz)\n","        embs = []\n","        for index in range(batches):\n","            nodes_batch = nodes[index*b_sz:(index+1)*b_sz]\n","            with torch.no_grad():\n","                embs_batch = self.gnn(nodes_batch)\n","            assert len(embs_batch) == len(nodes_batch)\n","            embs.append(embs_batch)\n","        assert len(embs) == batches\n","        embs = torch.cat(embs, 0)\n","        assert len(embs) == len(nodes)\n","        return embs.cpu().numpy()\n","\n","    def predict(self):\n","        if not self.supervised:\n","            print('GNN.predict() is only supported for supervised learning.')\n","            sys.exit(0)\n","        nodes = self.dl.nodes_train\n","        b_sz = 500\n","        batches = math.ceil(len(nodes) / b_sz)\n","        preds = []\n","        for index in range(batches):\n","            nodes_batch = nodes[index*b_sz:(index+1)*b_sz]\n","            with torch.no_grad():\n","                embs_batch = self.gnn(nodes_batch)\n","                logists = self.classification(embs_batch)\n","                _, predicts = torch.max(logists, 1)\n","                preds.append(predicts)\n","        assert len(preds) == batches\n","        preds = torch.cat(preds, 0)\n","        assert len(preds) == len(nodes)\n","        return preds.cpu().numpy()\n","\n","    def release_cuda_cache(self):\n","        torch.cuda.empty_cache()\n","\n","\n","class DataLoader(object):\n","    def __init__(self, adj_matrix, raw_features, labels, supervised, device):\n","        super(DataLoader, self).__init__()\n","        self.adj_matrix = adj_matrix\n","        # load adjacency list and node features\n","        self.adj_list = self.get_adj_list(adj_matrix)\n","        if raw_features is None:\n","            features = self.get_features()\n","        else:\n","            features = raw_features\n","        assert features.shape[0] == len(self.adj_list) == self.adj_matrix.shape[0]\n","        self.features = torch.FloatTensor(features).to(device)\n","        self.nodes_train = list(range(len(self.adj_list)))\n","        if supervised:\n","            self.labels = np.asarray(labels)\n","\n","    def get_adj_list(self, adj_matrix):\n","        \"\"\"build adjacency list from adjacency matrix\"\"\"\n","        adj_list = {}\n","        for i in range(adj_matrix.shape[0]):\n","            adj_list[i] = set(np.where(adj_matrix[i].toarray() != 0)[1])\n","        return adj_list\n","\n","    def get_features(self):\n","        \"\"\"\n","        When raw features are not available,\n","        build one-hot degree features from the adjacency list.\n","        \"\"\"\n","        max_degree = np.max(np.sum(self.adj_matrix != 0, axis=1))\n","        features = np.zeros((self.adj_matrix.shape[0], max_degree))\n","        for node, neighbors in self.adj_list.items():\n","            features[node, len(neighbors)-1] = 1\n","        return features\n","\n","\n","class Classification(nn.Module):\n","    def __init__(self, emb_size, num_classes):\n","        super(Classification, self).__init__()\n","        self.fc1 = nn.Linear(emb_size, 64)\n","        self.fc2 = nn.Linear(64, num_classes)\n","\n","    def forward(self, embeds):\n","        x = F.elu(self.fc1(embeds))\n","        x = F.elu(self.fc2(x))\n","        logists = torch.log_softmax(x, 1)\n","        return logists\n","\n","\n","class Unsup_Loss(object):\n","    \"\"\"docstring for UnsupervisedLoss\"\"\"\n","    def __init__(self, dl, device):\n","        super(Unsup_Loss, self).__init__()\n","        self.Q = 10\n","        self.N_WALKS = 4\n","        self.WALK_LEN = 4\n","        self.N_WALK_LEN = 5\n","        self.MARGIN = 3\n","        self.adj_lists = dl.adj_list\n","        self.adj_matrix = dl.adj_matrix\n","        self.train_nodes = dl.nodes_train\n","        self.device = device\n","\n","        self.target_nodes = None\n","        self.positive_pairs = []\n","        self.negative_pairs = []\n","        self.node_positive_pairs = {}\n","        self.node_negative_pairs = {}\n","        self.unique_nodes_batch = []\n","\n","    def get_loss_sage(self, embeddings, nodes):\n","        assert len(embeddings) == len(self.unique_nodes_batch)\n","        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n","        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n","\n","        nodes_score = []\n","        assert len(self.node_positive_pairs) == len(self.node_negative_pairs)\n","        for node in self.node_positive_pairs:\n","            pps = self.node_positive_pairs[node]\n","            nps = self.node_negative_pairs[node]\n","            if len(pps) == 0 or len(nps) == 0:\n","                continue\n","\n","            # Q * Exception(negative score)\n","            indexs = [list(x) for x in zip(*nps)]\n","            node_indexs = [node2index[x] for x in indexs[0]]\n","            neighb_indexs = [node2index[x] for x in indexs[1]]\n","            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n","            neg_score = self.Q*torch.mean(torch.log(torch.sigmoid(-neg_score)), 0)\n","\n","            # multiple positive score\n","            indexs = [list(x) for x in zip(*pps)]\n","            node_indexs = [node2index[x] for x in indexs[0]]\n","            neighb_indexs = [node2index[x] for x in indexs[1]]\n","            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n","            pos_score = torch.log(torch.sigmoid(pos_score))\n","\n","            nodes_score.append(torch.mean(- pos_score - neg_score).view(1,-1))\n","\n","        loss = torch.mean(torch.cat(nodes_score, 0))\n","        return loss\n","\n","    def get_loss_margin(self, embeddings, nodes):\n","        \n","        '''I commented these out bc it was throwing assertion error that I can't understand '''\n","#         assert len(embeddings) == len(self.unique_nodes_batch)\n","#         assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n","\n","\n","        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n","\n","        nodes_score = []\n","        \n","        \n","        ''' See above'''\n","#         assert len(self.node_positive_pairs) == len(self.node_negative_pairs) \n","        for node in self.node_positive_pairs:\n","            pps = self.node_positive_pairs[node]\n","            nps = self.node_negative_pairs[node]\n","            if len(pps) == 0 or len(nps) == 0:\n","                continue\n","\n","            indexs = [list(x) for x in zip(*pps)]\n","            node_indexs = [node2index[x] for x in indexs[0]]\n","            neighb_indexs = [node2index[x] for x in indexs[1]]\n","            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n","            pos_score, _ = torch.min(torch.log(torch.sigmoid(pos_score)), 0)\n","\n","            indexs = [list(x) for x in zip(*nps)]\n","            node_indexs = [node2index[x] for x in indexs[0]]\n","            neighb_indexs = [node2index[x] for x in indexs[1]]\n","            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n","            neg_score, _ = torch.max(torch.log(torch.sigmoid(neg_score)), 0)\n","\n","            nodes_score.append(torch.max(torch.tensor(0.0).to(self.device),\n","                                         neg_score-pos_score+self.MARGIN).view(1, -1))\n","        loss = torch.mean(torch.cat(nodes_score, 0), 0)\n","        return loss\n","\n","    def extend_nodes(self, nodes, num_neg=6):\n","        self.positive_pairs = []\n","        self.node_positive_pairs = {}\n","        self.negative_pairs = []\n","        self.node_negative_pairs = {}\n","\n","        self.target_nodes = nodes\n","        self.get_positive_nodes(nodes)\n","        self.get_negative_nodes(nodes, num_neg)\n","        self.unique_nodes_batch = list(set([i for x in self.positive_pairs for i in x])\n","                                       | set([i for x in self.negative_pairs for i in x]))\n","        assert set(self.target_nodes) < set(self.unique_nodes_batch)\n","        return self.unique_nodes_batch\n","\n","    def get_positive_nodes(self, nodes):\n","        return self._run_random_walks(nodes)\n","\n","    def get_negative_nodes(self, nodes, num_neg):\n","        for node in nodes:\n","            neighbors = set([node])\n","            frontier = set([node])\n","            for _ in range(self.N_WALK_LEN):\n","                current = set()\n","                for outer in frontier:\n","                    current |= self.adj_lists[int(outer)]\n","                frontier = current - neighbors\n","                neighbors |= current\n","            far_nodes = set(self.train_nodes) - neighbors\n","            neg_samples = random.sample(far_nodes, num_neg) if num_neg < len(far_nodes) else far_nodes\n","            self.negative_pairs.extend([(node, neg_node) for neg_node in neg_samples])\n","            self.node_negative_pairs[node] = [(node, neg_node) for neg_node in neg_samples]\n","        return self.negative_pairs\n","\n","    def _run_random_walks(self, nodes):\n","        for node in nodes:\n","            if len(self.adj_lists[int(node)]) == 0:\n","                continue\n","            cur_pairs = []\n","            for _ in range(self.N_WALKS):\n","                curr_node = node\n","                for _ in range(self.WALK_LEN):\n","                    cnts = self.adj_matrix[int(curr_node)].toarray().squeeze()\n","                    neighs = []\n","                    for n in np.where(cnts != 0)[0]:\n","                        neighs.extend([n] * int(cnts[n]))\n","                    # neighs = self.adj_lists[int(curr_node)]\n","                    next_node = random.choice(list(neighs))\n","                    # self co-occurrences are useless\n","                    if next_node != node and next_node in self.train_nodes:\n","                        self.positive_pairs.append((node,next_node))\n","                        cur_pairs.append((node,next_node))\n","                    curr_node = next_node\n","\n","            self.node_positive_pairs[node] = cur_pairs\n","        return self.positive_pairs\n","\n","\n","class SageLayer(nn.Module):\n","    \"\"\"\n","    Encodes a node's using 'convolutional' GraphSage approach\n","    \"\"\"\n","    def __init__(self, input_size, out_size, gat=False, gcn=False):\n","        super(SageLayer, self).__init__()\n","\n","        self.input_size = input_size\n","        self.out_size = out_size\n","\n","        self.gat = gat\n","        self.gcn = gcn\n","        self.weight = nn.Parameter(torch.FloatTensor(out_size, self.input_size if self.gat or self.gcn else 2 * self.input_size))\n","\n","        self.init_params()\n","\n","    def init_params(self):\n","        for param in self.parameters():\n","            nn.init.xavier_uniform_(param)\n","\n","    def forward(self, self_feats, aggregate_feats):\n","        \"\"\"\n","        Generates embeddings for a batch of nodes.\n","        nodes\t -- list of nodes\n","        \"\"\"\n","        if self.gat or self.gcn:\n","            combined = aggregate_feats\n","        else:\n","            combined = torch.cat([self_feats, aggregate_feats], dim=1)\n","        combined = F.relu(self.weight.mm(combined.t())).t()\n","        return combined\n","\n","class Attention(nn.Module):\n","    \"\"\"Computes the self-attention between pair of nodes\"\"\"\n","    def __init__(self, input_size, out_size):\n","        super(Attention, self).__init__()\n","\n","        self.input_size = input_size\n","        self.out_size = out_size\n","        self.attention_raw = nn.Linear(2*input_size, 1, bias=False)\n","        self.attention_emb = nn.Linear(2*out_size, 1, bias=False)\n","\n","    def forward(self, row_embs, col_embs):\n","        if row_embs.size(1) == self.input_size:\n","            att = self.attention_raw\n","        elif row_embs.size(1) == self.out_size:\n","            att = self.attention_emb\n","        e = att(torch.cat((row_embs, col_embs), dim=1))\n","        return F.leaky_relu(e, negative_slope=0.2)\n","\n","class GNN_model(nn.Module):\n","    \"\"\"docstring for GraphSage\"\"\"\n","    def __init__(self, num_layers, out_size, dl, device, gat=False, gcn=False, agg_func='MEAN'):\n","        super(GNN_model, self).__init__()\n","\n","        self.input_size = dl.features.size(1)\n","        self.out_size = out_size\n","        self.num_layers = num_layers\n","        self.gat = gat\n","        self.gcn = gcn\n","        self.device = device\n","        self.agg_func = agg_func\n","\n","        self.raw_features = dl.features\n","        self.adj_lists = dl.adj_list\n","        self.adj_matrix = dl.adj_matrix\n","\n","        for index in range(1, num_layers+1):\n","            layer_size = out_size if index != 1 else self.input_size\n","            setattr(self, 'sage_layer'+str(index), SageLayer(layer_size, out_size, gat=self.gat, gcn=self.gcn))\n","        if self.gat:\n","            self.attention = Attention(self.input_size, out_size)\n","\n","    def forward(self, nodes_batch):\n","        \"\"\"\n","        Generates embeddings for a batch of nodes.\n","        nodes_batch\t-- batch of nodes to learn the embeddings\n","        \"\"\"\n","        lower_layer_nodes = list(nodes_batch)\n","        nodes_batch_layers = [(lower_layer_nodes,)]\n","        for _ in range(self.num_layers):\n","            lower_layer_nodes, lower_samp_neighs, lower_layer_nodes_dict= self._get_unique_neighs_list(lower_layer_nodes)\n","            nodes_batch_layers.insert(0, (lower_layer_nodes, lower_samp_neighs, lower_layer_nodes_dict))\n","\n","        assert len(nodes_batch_layers) == self.num_layers + 1\n","\n","        pre_hidden_embs = self.raw_features\n","        for index in range(1, self.num_layers+1):\n","            nb = nodes_batch_layers[index][0]\n","            pre_neighs = nodes_batch_layers[index-1]\n","            aggregate_feats = self.aggregate(nb, pre_hidden_embs, pre_neighs)\n","            sage_layer = getattr(self, 'sage_layer'+str(index))\n","            if index > 1:\n","                nb = self._nodes_map(nb, pre_neighs)\n","            cur_hidden_embs = sage_layer(self_feats=pre_hidden_embs[nb], aggregate_feats=aggregate_feats)\n","            pre_hidden_embs = cur_hidden_embs\n","\n","        return pre_hidden_embs\n","\n","    def _nodes_map(self, nodes, neighs):\n","        _, samp_neighs, layer_nodes_dict = neighs\n","        assert len(samp_neighs) == len(nodes)\n","        index = [layer_nodes_dict[x] for x in nodes]\n","        return index\n","\n","    def _get_unique_neighs_list(self, nodes, num_sample=10):\n","        _set = set\n","        to_neighs = [self.adj_lists[int(node)] for node in nodes]\n","        if self.gcn or self.gat:\n","            samp_neighs = to_neighs\n","        else:\n","            _sample = random.sample\n","            samp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n","        samp_neighs = [samp_neigh | set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n","        _unique_nodes_list = list(set.union(*samp_neighs))\n","        i = list(range(len(_unique_nodes_list)))\n","        # unique node 2 index\n","        unique_nodes = dict(list(zip(_unique_nodes_list, i)))\n","        return _unique_nodes_list, samp_neighs, unique_nodes\n","\n","    def aggregate(self, nodes, pre_hidden_embs, pre_neighs):\n","        unique_nodes_list, samp_neighs, unique_nodes = pre_neighs\n","\n","        assert len(nodes) == len(samp_neighs)\n","        indicator = [(nodes[i] in samp_neighs[i]) for i in range(len(samp_neighs))]\n","        assert False not in indicator\n","        if not self.gat and not self.gcn:\n","            samp_neighs = [(samp_neighs[i]-set([nodes[i]])) for i in range(len(samp_neighs))]\n","        if len(pre_hidden_embs) == len(unique_nodes):\n","            embed_matrix = pre_hidden_embs\n","        else:\n","            embed_matrix = pre_hidden_embs[torch.LongTensor(unique_nodes_list)]\n","        # get row and column nonzero indices for the mask tensor\n","        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n","        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n","        # get the edge counts for each edge\n","        edge_counts = self.adj_matrix[nodes][:, unique_nodes_list].toarray()\n","        edge_counts = torch.FloatTensor(edge_counts).to(embed_matrix.device)\n","        torch.sqrt_(edge_counts)\n","        if self.gat:\n","            indices = (torch.LongTensor(row_indices), torch.LongTensor(column_indices))\n","            nodes_indices = torch.LongTensor([unique_nodes[nodes[n]] for n in row_indices])\n","            row_embs = embed_matrix[nodes_indices]\n","            col_embs = embed_matrix[column_indices]\n","            atts = self.attention(row_embs, col_embs).squeeze()\n","            mask = torch.zeros(len(samp_neighs), len(unique_nodes)).to(embed_matrix.device)\n","            mask.index_put_(indices, atts)\n","            mask = mask * edge_counts\n","            # softmax\n","            mask = torch.exp(mask) * (mask != 0).float()\n","            mask = F.normalize(mask, p=1, dim=1)\n","        else:\n","            mask = torch.zeros(len(samp_neighs), len(unique_nodes)).to(embed_matrix.device)\n","            mask[row_indices, column_indices] = 1\n","            # multiply edge counts to mask\n","            mask = mask * edge_counts\n","            mask = F.normalize(mask, p=1, dim=1)\n","            mask = mask.to(embed_matrix.device)\n","\n","        if self.agg_func == 'MEAN':\n","            aggregate_feats = mask.mm(embed_matrix)\n","        elif self.agg_func == 'MAX':\n","            indexs = [x.nonzero() for x in mask != 0]\n","            aggregate_feats = []\n","            for feat in [embed_matrix[x.squeeze()] for x in indexs]:\n","                if len(feat.size()) == 1:\n","                    aggregate_feats.append(feat.view(1, -1))\n","                else:\n","                    aggregate_feats.append(torch.max(feat,0)[0].view(1, -1))\n","            aggregate_feats = torch.cat(aggregate_feats, 0)\n","\n","        return aggregate_feats"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":168125,"status":"ok","timestamp":1645190107904,"user":{"displayName":"Khaled Saifuddin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2bTbFy6IGkt9EcfgA2kz0wTOlLTj78J8wQOP6=s64","userId":"17491546006244706959"},"user_tz":360},"id":"4fRl73wKcBBo","outputId":"ea054e29-9b79-439b-9d60-c76eea19ea42"},"outputs":[{"name":"stderr","output_type":"stream","text":[""]}],"source":["\n","gcn =GNN(line_Adj, epochs=5, supervised=False, model='gcn')\n","graphsage =GNN(line_Adj, epochs=5, supervised=False, model='graphsage')\n","gat =GNN(line_Adj, epochs=5, supervised=False, model='gat')\n","\n","gcn.fit()\n","graphsage.fit()\n","gat.fit()\n","\n","embs_gcn = gcn.generate_embeddings()\n","embs_graphsage = graphsage.generate_embeddings()\n","embs_gat = gat.generate_embeddings()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDM7ol1xf-3H"},"outputs":[],"source":["embs_gcn=torch.from_numpy(embs_gcn)\n","embs_graphsage=torch.from_numpy(embs_graphsage)\n","embs_gat=torch.from_numpy(embs_gat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPQTVwuKeq5h"},"outputs":[],"source":["pos_score_train = decoder(train_pos_g, embs_gcn)\n","neg_score_train = decoder(train_neg_g, embs_gcn)\n","pos_score_test = decoder(test_pos_g, embs_gcn)\n","neg_score_test = decoder(test_neg_g, embs_gcn)\n","\n","\n","scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n","labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n","scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n","labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117,"status":"ok","timestamp":1645190285647,"user":{"displayName":"Khaled Saifuddin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2bTbFy6IGkt9EcfgA2kz0wTOlLTj78J8wQOP6=s64","userId":"17491546006244706959"},"user_tz":360},"id":"etzxIYDp-ux5","outputId":"5195ff1b-2562-410d-9877-ca2a597f63e4"},"outputs":[{"data":{"text/plain":["(38700, 256)"]},"execution_count":168,"metadata":{},"output_type":"execute_result"}],"source":["scores_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45253,"status":"ok","timestamp":1645190182418,"user":{"displayName":"Khaled Saifuddin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2bTbFy6IGkt9EcfgA2kz0wTOlLTj78J8wQOP6=s64","userId":"17491546006244706959"},"user_tz":360},"id":"qmxcfggYgHLP","outputId":"0ff87deb-cb74-450f-8570-c00bc7bd7427"},"outputs":[{"name":"stdout","output_type":"stream","text":["LR\n","accuracy:  0.541266149870801 precision:  0.5508851764272542 recall:  0.541266149870801 f1-score:  0.6231825610765606 ROC-AUC:  0.5412661498708009 PR-AUC:  0.7040447331934078\n","NB\n","accuracy:  0.5448837209302325 precision:  0.5902227884768768 recall:  0.5448837209302325 f1-score:  0.6639830589312627 ROC-AUC:  0.5448837209302325 PR-AUC:  0.7379644694441925\n","RF\n","accuracy:  0.546046511627907 precision:  0.5846158397636402 recall:  0.546046511627907 f1-score:  0.6606135538211885 ROC-AUC:  0.546046511627907 PR-AUC:  0.7346484539729777\n"]}],"source":["print(\"LR\")\n","lr = LogisticRegression(class_weight=\"balanced\")\n","lr.fit(scores_train, labels_train)\n","predictions = lr.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"NB\")\n","gnb = GaussianNB()\n","gnb.fit(scores_train, labels_train)\n","predictions = gnb.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"RF\")\n","clf = RandomForestClassifier(max_depth=2, random_state=0)\n","clf.fit(scores_train, labels_train)\n","predictions = clf.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wItvEWGIwwi_"},"outputs":[],"source":["pos_score_train = decoder(train_pos_g, embs_graphsage)\n","neg_score_train = decoder(train_neg_g, embs_graphsage)\n","pos_score_test = decoder(test_pos_g, embs_graphsage)\n","neg_score_test = decoder(test_neg_g, embs_graphsage)\n","\n","\n","scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n","labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n","scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n","labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51464,"status":"ok","timestamp":1645190382241,"user":{"displayName":"Khaled Saifuddin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2bTbFy6IGkt9EcfgA2kz0wTOlLTj78J8wQOP6=s64","userId":"17491546006244706959"},"user_tz":360},"id":"vZOc3Dots08h","outputId":"92617148-7e46-4476-c8b6-699a7a487554"},"outputs":[{"name":"stdout","output_type":"stream","text":["LR\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"name":"stdout","output_type":"stream","text":["accuracy:  0.5731524547803618 precision:  0.5733092898367258 recall:  0.5731524547803618 f1-score:  0.5828008586942797 ROC-AUC:  0.5731524547803618 PR-AUC:  0.6840290166391932\n","NB\n","accuracy:  0.5630232558139535 precision:  0.5725284939049564 recall:  0.5630232558139535 f1-score:  0.6299967180833607 ROC-AUC:  0.5630232558139534 PR-AUC:  0.7091437792572788\n","RF\n","accuracy:  0.5724289405684755 precision:  0.5778336918858435 recall:  0.5724289405684755 f1-score:  0.6222059864380465 ROC-AUC:  0.5724289405684754 PR-AUC:  0.704708215583079\n"]}],"source":["print(\"LR\")\n","lr = LogisticRegression()\n","lr.fit(scores_train, labels_train)\n","predictions = lr.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"NB\")\n","gnb = GaussianNB()\n","gnb.fit(scores_train, labels_train)\n","predictions = gnb.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"RF\")\n","clf = RandomForestClassifier(max_depth=2, random_state=0)\n","clf.fit(scores_train, labels_train)\n","predictions = clf.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K84WhItpwy2n"},"outputs":[],"source":["pos_score_train = decoder(train_pos_g, embs_gat)\n","neg_score_train = decoder(train_neg_g, embs_gat)\n","pos_score_test = decoder(test_pos_g, embs_gat)\n","neg_score_test = decoder(test_neg_g, embs_gat)\n","\n","\n","scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n","labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n","scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n","labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39629,"status":"ok","timestamp":1645190421840,"user":{"displayName":"Khaled Saifuddin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2bTbFy6IGkt9EcfgA2kz0wTOlLTj78J8wQOP6=s64","userId":"17491546006244706959"},"user_tz":360},"id":"6MooBWrlw1ql","outputId":"7cc79ad0-ca68-4a90-c543-603f9669d63b"},"outputs":[{"name":"stdout","output_type":"stream","text":["LR\n","accuracy:  0.538062015503876 precision:  0.5466821348139691 recall:  0.538062015503876 f1-score:  0.6197596511751569 ROC-AUC:  0.538062015503876 PR-AUC:  0.7015410169941123\n","NB\n","accuracy:  0.5384496124031007 precision:  0.5715021618621324 recall:  0.5384496124031007 f1-score:  0.6555461277383523 ROC-AUC:  0.5384496124031007 PR-AUC:  0.731043524304239\n","RF\n","accuracy:  0.5459689922480621 precision:  0.5869585920668676 recall:  0.5459689922480621 f1-score:  0.6619986534577282 ROC-AUC:  0.545968992248062 PR-AUC:  0.7359406682399366\n"]}],"source":["print(\"LR\")\n","lr = LogisticRegression(class_weight=\"balanced\")\n","lr.fit(scores_train, labels_train)\n","predictions = lr.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"NB\")\n","gnb = GaussianNB()\n","gnb.fit(scores_train, labels_train)\n","predictions = gnb.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n","\n","print(\"RF\")\n","clf = RandomForestClassifier(max_depth=2, random_state=0)\n","clf.fit(scores_train, labels_train)\n","predictions = clf.predict(scores_test)\n","precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n","auc_precision_recall = auc(recall, precision)\n","print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102,"status":"ok","timestamp":1645188403555,"user":{"displayName":"Khaled Saifuddin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2bTbFy6IGkt9EcfgA2kz0wTOlLTj78J8wQOP6=s64","userId":"17491546006244706959"},"user_tz":360},"id":"FVFAqssN3etf","outputId":"dbc7ee26-8bbe-471a-f549-dee0ba2d9895"},"outputs":[{"data":{"text/plain":["array([0.5       , 0.52590692, 1.        ])"]},"execution_count":149,"metadata":{},"output_type":"execute_result"}],"source":["precision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"42z3w3gV1Cyh"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fppar1Js1CeN"},"outputs":[],"source":["'''\n","here, the hyG creation starts\n","1. at frist we read the text file \"new_graph_data_1.txt\". This text file is created by running ESPF algorithm\n","2. Then we create a dictionary 'DICT' that has 'drug' as key and drug's 'substructures' as values \n","'''\n","formattedNodes = []\n","smileFile = open(\"drive/My Drive/Colab Notebooks/Hypergraph_my/new_graph_data.txt\", 'r')\n","names = []\n","\n","for line in smileFile:\n","    line = line.strip()\n","    words = line.split(\" \")\n","    name = words[0]\n","    del words[0]\n","    t=tuple(words)\n","    names.append(name)\n","    formattedNodes.append(t)\n","      \n","# Make the dictionary of the Drug IDs: Smiles tokens    \n","DICT = dict()  \n","for i in range(0, len(names)):\n","    DICT[names[i]] = formattedNodes[i]    \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3O-mediZiJE"},"outputs":[],"source":["# Pass the dict to create graph\n","H = hnx.Hypergraph(DICT)\n","incidence=H.incidence_matrix()\n","\"\"\"\n","This below line graph/regular graph is generated from the hyG that will be used in GCN\n","\"\"\"\n","H1 = H.convert_to_static(H)\n","Hline = H1.get_linegraph(s = 2, edges=True, use_nwhy=True)\n","line_Adj = nx.adjacency_matrix(Hline)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nNdQwHDhdmB0"},"outputs":[],"source":["line_Adj = nx.to_scipy_sparse_matrix(Hline, format = 'csr')\n","line_Adj=line_Adj.toarray()\n","np.fill_diagonal(line_Adj, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxVpfnZtj5Rp"},"outputs":[],"source":["from scipy import sparse\n","line_Adj= sparse.csr_matrix(line_Adj) "]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Using HAT.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}